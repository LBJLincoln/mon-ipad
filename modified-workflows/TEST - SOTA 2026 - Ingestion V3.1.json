{
  "name": "TEST - SOTA 2026 - Ingestion V3.1",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "rag-v6-ingestion",
        "options": {
          "rawBody": true
        }
      },
      "id": "0a8edc75-fde0-476b-9ac0-1fb6ac34db3b",
      "name": "S3 Event Webhook",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2.1,
      "position": [
        -3552,
        208
      ],
      "webhookId": "be3d4b15-3f57-4069-8f12-7ba3b122b0ae"
    },
    {
      "parameters": {
        "jsCode": "// Shield #1: Distributed Lock + Trace Init\nconst crypto = require('crypto');\nconst body = $json.body || $json || {};\n\nif (!body.objectKey && !body.key) {\n  throw new Error('VALIDATION_ERROR: objectKey is required');\n}\n\nconst objectKey = body.objectKey || body.key;\nconst hash = crypto.createHash('sha256').update(objectKey).digest('hex');\nconst workerId = `worker-${$execution.id}`;\nconst lockKey = `lock:ingestion:${hash}`;\nconst traceId = `tr-ingest-${Date.now()}-${Math.random().toString(36).substring(2, 10)}`;\n\n// Extract file extension for MIME routing\nconst ext = (objectKey.split('.').pop() || '').toLowerCase();\n\nreturn {\n  lockKey,\n  workerId,\n  hash,\n  objectKey,\n  file_extension: ext,\n  bucket: body.bucket || 'default',\n  traceId,\n  timestamp: new Date().toISOString(),\n  s3_url: body.s3_url || `s3://${body.bucket || 'default'}/${objectKey}`,\n  ttl_seconds: 3600,\n  tenant_id: body.tenant_id || 'default'\n};"
      },
      "id": "cb248158-002b-4b44-aa3e-edaf46cd57a1",
      "name": "Init Lock & Trace",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -3296,
        208
      ]
    },
    {
      "parameters": {
        "operation": "set",
        "key": "={{ $json.lockKey }}",
        "value": "={{ $json.workerId }}",
        "expire": true,
        "ttl": 3600
      },
      "id": "94420020-37da-45bc-8c8e-98017b59d58a",
      "name": "Redis: Acquire Lock",
      "type": "n8n-nodes-base.redis",
      "typeVersion": 1,
      "position": [
        -3104,
        208
      ],
      "credentials": {
        "redis": {
          "id": "O2KEPiv7VzgDG5ZX",
          "name": "Redis Upstash"
        }
      },
      "onError": "continueErrorOutput"
    },
    {
      "parameters": {
        "jsCode": "// Handle lock acquisition result\nconst lockInfo = $node['Init Lock & Trace'].json;\nconst redisResult = $json;\n\nconst lockAcquired = redisResult === 'OK' || redisResult === true || redisResult?.result === 'OK';\n\nif (!lockAcquired) {\n  console.warn('DUPLICATE_DOCUMENT', { lockKey: lockInfo.lockKey, trace_id: lockInfo.traceId });\n  return {\n    ...lockInfo,\n    lockAcquired: false,\n    status: 'DUPLICATE',\n    action: 'SKIP'\n  };\n}\n\nreturn {\n  ...lockInfo,\n  lockAcquired: true,\n  status: 'PROCESSING',\n  lockValue: lockInfo.workerId\n};"
      },
      "id": "86cd087e-f225-4202-abcb-3f870ae69f4d",
      "name": "Lock Result Handler",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -2896,
        208
      ]
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "typeValidation": "strict"
          },
          "conditions": [
            {
              "id": "lock-acquired",
              "leftValue": "={{ $json.lockAcquired }}",
              "rightValue": true,
              "operator": {
                "type": "boolean",
                "operation": "equals"
              }
            }
          ]
        },
        "options": {}
      },
      "id": "00c6bf7f-9cbc-4bdb-b30e-a6b24eac8155",
      "name": "Lock Acquired?",
      "type": "n8n-nodes-base.if",
      "position": [
        -2704,
        208
      ],
      "typeVersion": 2
    },
    {
      "parameters": {
        "jsCode": "// PATCH 5.1: MIME Type Detector & Router\nconst lockData = $json;\nconst ext = lockData.file_extension;\n\n// Quality scores by type (PATCH 5.1)\nconst MIME_CONFIG = {\n  'xlsx': { type: 'EXCEL', quality_score: 0.95, chunking_method: 'tabular', strategy: 'hi_res' },\n  'xls':  { type: 'EXCEL', quality_score: 0.95, chunking_method: 'tabular', strategy: 'hi_res' },\n  'csv':  { type: 'EXCEL', quality_score: 0.90, chunking_method: 'tabular', strategy: 'fast' },\n  'pdf':  { type: 'PDF', quality_score: 0.90, chunking_method: 'semantic', strategy: 'hi_res' },\n  'docx': { type: 'WORD', quality_score: 0.85, chunking_method: 'semantic', strategy: 'hi_res' },\n  'doc':  { type: 'WORD', quality_score: 0.80, chunking_method: 'semantic', strategy: 'hi_res' },\n  'pptx': { type: 'POWERPOINT', quality_score: 0.70, chunking_method: 'slide', strategy: 'hi_res' },\n  'ppt':  { type: 'POWERPOINT', quality_score: 0.65, chunking_method: 'slide', strategy: 'hi_res' },\n  'txt':  { type: 'TEXT', quality_score: 0.60, chunking_method: 'semantic', strategy: 'fast' },\n  'md':   { type: 'TEXT', quality_score: 0.65, chunking_method: 'semantic', strategy: 'fast' },\n  'html': { type: 'TEXT', quality_score: 0.55, chunking_method: 'semantic', strategy: 'fast' }\n};\n\nconst config = MIME_CONFIG[ext] || { type: 'UNKNOWN', quality_score: 0.50, chunking_method: 'semantic', strategy: 'fast' };\n\nreturn {\n  ...lockData,\n  mime_type: config.type,\n  quality_score: config.quality_score,\n  chunking_method: config.chunking_method,\n  extraction_strategy: config.strategy\n};"
      },
      "id": "60cff024-250d-4898-b64c-67d44e484a39",
      "name": "MIME Type Detector",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -2496,
        112
      ],
      "notes": "PATCH 5.1: Route selon type fichier"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.unstructured.io/general/v0/general",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "unstructured-api-key",
              "value": "={{$credentials.unstructured_api}}"
            }
          ]
        },
        "sendBody": true,
        "contentType": "multipart-form-data",
        "bodyParameters": {
          "parameters": [
            {
              "name": "files",
              "value": "={{$json.s3_url}}"
            },
            {
              "name": "strategy",
              "value": "={{$json.extraction_strategy}}"
            },
            {
              "name": "pdf_infer_table_structure",
              "value": "true"
            },
            {
              "name": "extract_image_block_types",
              "value": "[\"Image\", \"Table\"]"
            }
          ]
        },
        "options": {
          "timeout": 90000
        }
      },
      "id": "678621d0-82c3-4df1-9269-b2ecb094f07c",
      "name": "OCR Extraction",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.3,
      "position": [
        -2256,
        112
      ],
      "onError": "continueErrorOutput"
    },
    {
      "parameters": {
        "jsCode": "// Shield #5: PII Fortress - Linear Regex Only\nconst MAX_CHUNK_SIZE = 50000;\nconst startTime = Date.now();\n\nconst content = $input.all().map(item => {\n  let text = item.json.text || item.json.content || '';\n  if (text.length > MAX_CHUNK_SIZE) text = text.substring(0, MAX_CHUNK_SIZE);\n  \n  let piiDetected = [];\n  text = text.normalize('NFD').replace(/[\\u0300-\\u036f]/g, '');\n  \n  // Email\n  const emailRegex = /[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6}/g;\n  const emails = text.match(emailRegex);\n  if (emails) {\n    piiDetected.push({ type: 'EMAIL', count: emails.length });\n    text = text.replace(emailRegex, '[EMAIL_REDACTED]');\n  }\n  \n  // Phone FR\n  const phoneFRRegex = /(?:0|\\+33)[1-9](?:[\\s.-]?[0-9]{2}){4}/g;\n  const phones = text.match(phoneFRRegex);\n  if (phones) {\n    piiDetected.push({ type: 'PHONE_FR', count: phones.length });\n    text = text.replace(phoneFRRegex, '[PHONE_REDACTED]');\n  }\n  \n  // IBAN\n  const ibanRegex = /[A-Z]{2}[0-9]{2}[A-Z0-9]{10,30}/gi;\n  const ibans = text.match(ibanRegex);\n  if (ibans) {\n    piiDetected.push({ type: 'IBAN', count: ibans.length });\n    text = text.replace(ibanRegex, '[IBAN_REDACTED]');\n  }\n  \n  // Credit Card\n  const ccRegex = /[0-9]{4}[\\s-]?[0-9]{4}[\\s-]?[0-9]{4}[\\s-]?[0-9]{4}/g;\n  const ccs = text.match(ccRegex);\n  if (ccs) {\n    piiDetected.push({ type: 'CREDIT_CARD', count: ccs.length });\n    text = text.replace(ccRegex, '[CC_REDACTED]');\n  }\n  \n  return { \n    ...item.json, \n    processed_content: text,\n    pii_detected: piiDetected,\n    pii_count: piiDetected.reduce((sum, p) => sum + p.count, 0)\n  };\n});\n\nreturn content;"
      },
      "id": "3c5f0b7b-c5d0-4185-8d6f-a44402f3e432",
      "name": "PII Fortress",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -2000,
        112
      ]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "={{ $vars.LLM_API_URL || 'https://api.openai.com/v1/chat/completions' }}",
        "authentication": "genericCredentialType",
        "genericAuthType": "httpHeaderAuth",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={\n  \"model\": \"{{ $vars.CHUNKING_MODEL || 'deepseek-chat' }}\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"Tu es un expert en decoupe semantique adaptative.\\n\\n=== STRATEGIE SELON TYPE DE CONTENU ===\\n\\nNARRATIF (rapports, articles):\\n- Chunks de 300-500 mots\\n- Decoupe aux changements de sujet/ton\\n\\nTECHNIQUE (documentation, specs):\\n- Chunks de 200-400 mots\\n- Decoupe aux sections logiques\\n\\nTABULAIRE (donnees, listes):\\n- Garder les tableaux entiers si possible\\n- Sinon, grouper les lignes liees\\n\\nLEGAL (contrats, policies):\\n- Chunks plus longs (500-800 mots)\\n- Ne jamais couper une clause\\n\\n=== REGLES ===\\n1. Detecte le type de contenu\\n2. Applique la strategie appropriee\\n3. Chaque chunk doit etre coherent et auto-suffisant\\n4. Indique le topic de chaque chunk\\n\\nReponds JSON: {\\\"content_type\\\": \\\"narrative|technical|tabular|legal\\\", \\\"chunks\\\": [{\\\"content\\\": string, \\\"topic\\\": string, \\\"start_index\\\": number}]}\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Type MIME: {{ $node['MIME Type Detector'].json.mime_type }}\\nNom fichier: {{ $node['MIME Type Detector'].json.objectKey }}\\n\\nContenu:\\n{{ $json.processed_content?.substring(0, 15000) || '' }}\"\n    }\n  ],\n  \"temperature\": 0.1,\n  \"max_tokens\": 4000,\n  \"response_format\": { \"type\": \"json_object\" }\n}",
        "options": {
          "timeout": 60000
        }
      },
      "id": "d896e3ff-45d1-4fff-9def-f98b85eb695e",
      "name": "Semantic Chunker V3.1 (Adaptive)",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.3,
      "position": [
        -1744,
        112
      ],
      "credentials": {
        "httpHeaderAuth": {
          "id": "3DEiHDwB09D65919",
          "name": "Pinecone API Key"
        }
      },
      "notes": "PATCH 5.2: LLM-based semantic chunking",
      "retryOnFail": true,
      "maxTries": 3,
      "waitBetweenTries": 2000
    },
    {
      "parameters": {
        "jsCode": "// PATCH P04: Chunk Validator & Fallback\n// Includes RecursiveCharacterTextSplitter fallback and chunk validation\n// Impact: Prevents empty/invalid chunks from reaching downstream nodes\n// Reference: ARCHITECTURE_FINALE_SOTA_2026.md section 3.5\n\nconst crypto = require('crypto');\nconst mimeData = $node['MIME Type Detector'].json;\nconst piiData = $node['PII Fortress'].json;\nconst fullContent = piiData.processed_content || '';\n\nlet chunksData;\ntry {\n  chunksData = JSON.parse($json.choices?.[0]?.message?.content || '{}');\n} catch (e) {\n  chunksData = null;\n}\n\nlet chunks = chunksData?.chunks || [];\n\n// === VALIDATION ===\nconst MIN_CHUNK_SIZE = 50;    // Minimum characters per chunk\nconst MAX_CHUNK_SIZE = 3000;  // Maximum characters per chunk\n\nconst isValid = chunks.length > 0 && chunks.every(c =>\n  c.content &&\n  c.content.length >= MIN_CHUNK_SIZE &&\n  c.content.length <= MAX_CHUNK_SIZE * 2\n);\n\n// === FALLBACK: RecursiveCharacterTextSplitter ===\nif (!isValid || chunks.length === 0) {\n  console.warn('LLM chunking failed or invalid, falling back to recursive splitter');\n\n  const CHUNK_SIZE = 800;\n  const OVERLAP = 200;\n  const SEPARATORS = ['\\n\\n', '\\n', '. ', '! ', '? ', '; ', ', ', ' '];\n\n  function recursiveSplit(text, separators, chunkSize, overlap) {\n    if (text.length <= chunkSize) return [text];\n\n    const results = [];\n    let currentSep = separators[0] || ' ';\n\n    for (const sep of separators) {\n      if (text.includes(sep)) {\n        currentSep = sep;\n        break;\n      }\n    }\n\n    const parts = text.split(currentSep);\n    let currentChunk = '';\n\n    for (const part of parts) {\n      const candidate = currentChunk ? currentChunk + currentSep + part : part;\n\n      if (candidate.length > chunkSize && currentChunk) {\n        results.push(currentChunk.trim());\n        const overlapText = currentChunk.substring(Math.max(0, currentChunk.length - overlap));\n        currentChunk = overlapText + currentSep + part;\n      } else {\n        currentChunk = candidate;\n      }\n    }\n\n    if (currentChunk.trim()) {\n      results.push(currentChunk.trim());\n    }\n\n    return results;\n  }\n\n  const splitTexts = recursiveSplit(fullContent, SEPARATORS, CHUNK_SIZE, OVERLAP);\n  chunks = splitTexts.map((text, idx) => ({\n    content: text,\n    topic: 'auto-split',\n    start_index: fullContent.indexOf(text)\n  }));\n}\n\n// === POST-VALIDATION: split oversized, merge undersized ===\nconst validatedChunks = [];\nfor (const chunk of chunks) {\n  if (chunk.content.length > MAX_CHUNK_SIZE) {\n    // Split oversized chunk at nearest sentence boundary\n    const mid = chunk.content.lastIndexOf('. ', Math.floor(chunk.content.length / 2));\n    const splitPoint = mid > 0 ? mid + 2 : Math.floor(chunk.content.length / 2);\n    validatedChunks.push({ ...chunk, content: chunk.content.substring(0, splitPoint) });\n    validatedChunks.push({ ...chunk, content: chunk.content.substring(splitPoint), topic: chunk.topic + ' (cont.)' });\n  } else if (chunk.content.length < MIN_CHUNK_SIZE && validatedChunks.length > 0) {\n    // Merge undersized chunk with previous\n    const prev = validatedChunks[validatedChunks.length - 1];\n    prev.content += '\\n' + chunk.content;\n  } else {\n    validatedChunks.push(chunk);\n  }\n}\n\n// === ENRICH CHUNKS ===\nconst parentId = crypto.createHash('sha256').update(mimeData.objectKey).digest('hex').substring(0, 32);\nconst documentTitle = mimeData.objectKey.split('/').pop().replace(/\\.[^/.]+$/, '');\nconst documentType = mimeData.mime_type || 'UNKNOWN';\n\n// Detect document sections/structure\nconst sections = [];\nconst sectionRegex = /^(#{1,3}\\s+|\\d+\\.\\s+|[A-Z][A-Z\\s]+:)/gm;\nlet match;\nwhile ((match = sectionRegex.exec(fullContent)) !== null) {\n  sections.push({ index: match.index, header: match[0].trim() });\n}\n\nconst enrichedChunks = validatedChunks.map((chunk, idx) => {\n  const chunkId = `${parentId}-chunk-${idx}`;\n\n  let currentSection = 'Introduction';\n  for (const section of sections) {\n    if (section.index <= (chunk.start_index || 0)) {\n      currentSection = section.header;\n    } else break;\n  }\n\n  return {\n    id: chunkId,\n    content: chunk.content,\n    contextual_content: chunk.content,  // Will be enriched by P01 Contextual Retrieval\n    contextual_prefix: '',               // Will be enriched by P01 Contextual Retrieval\n    topic: chunk.topic,\n    section: currentSection,\n    parent_id: parentId,\n    parent_filename: mimeData.objectKey,\n    document_title: documentTitle,\n    document_type: documentType,\n    quality_score: mimeData.quality_score,\n    version: 1,\n    is_obsolete: false,\n    chunk_method: isValid ? mimeData.chunking_method : 'recursive_fallback',\n    chunk_index: idx,\n    total_chunks: validatedChunks.length,\n    tenant_id: mimeData.tenant_id,\n    trace_id: mimeData.traceId,\n    pii_count: piiData.pii_count || 0,\n    created_at: new Date().toISOString()\n  };\n});\n\nreturn {\n  chunks: enrichedChunks,\n  full_document_text: fullContent,  // Passed to P01 for contextual retrieval\n  parent_id: parentId,\n  parent_filename: mimeData.objectKey,\n  document_title: documentTitle,\n  total_chunks: enrichedChunks.length,\n  trace_id: mimeData.traceId,\n  tenant_id: mimeData.tenant_id,\n  lock_key: mimeData.lockKey,\n  lock_value: mimeData.lockValue,\n  chunking_method: isValid ? 'llm_semantic' : 'recursive_fallback',\n  sections_detected: sections.length\n};"
      },
      "id": "bbce1a3c-6fb9-475e-8ec8-36d8a78cbfa4",
      "name": "Chunk Validator & Enricher V4",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -1504,
        112
      ],
      "notes": "PATCH P04: Chunk validation + RecursiveCharacterTextSplitter fallback + V4 enrichment"
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "UPDATE documents SET is_obsolete = true, obsoleted_at = NOW(), superseded_by = $1 WHERE parent_filename = $2 AND tenant_id = $3 AND is_obsolete = false",
        "options": {}
      },
      "id": "f0dc58cd-9c39-4852-a5ec-e1296f8c1fa6",
      "name": "Version Manager",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.4,
      "position": [
        -1248,
        112
      ],
      "credentials": {
        "postgres": {
          "id": "FZUFrHg9RgDR3MAB",
          "name": "Postgres Production"
        }
      },
      "onError": "continueErrorOutput",
      "notes": "PATCH 5.4: Mark old versions obsolete"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "={{ $vars.LLM_API_URL || 'https://api.openai.com/v1/chat/completions' }}",
        "authentication": "genericCredentialType",
        "genericAuthType": "httpHeaderAuth",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={\n  \"model\": \"{{ $vars.QA_MODEL || 'deepseek-chat' }}\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"Genere exactement 3 questions auxquelles ce paragraphe repond. Format JSON: {\\\"questions\\\": [\\\"Q1?\\\", \\\"Q2?\\\", \\\"Q3?\\\"]}\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"{{ JSON.stringify($json.chunks?.slice(0, 5).map(c => c.content.substring(0, 500))) }}\"\n    }\n  ],\n  \"temperature\": 0.3,\n  \"max_tokens\": 500,\n  \"response_format\": { \"type\": \"json_object\" }\n}",
        "options": {
          "timeout": 30000
        }
      },
      "id": "d8aefa02-35c3-4cd2-8757-4eda27da4e29",
      "name": "Q&A Generator",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.3,
      "position": [
        -1008,
        112
      ],
      "credentials": {
        "httpHeaderAuth": {
          "id": "3DEiHDwB09D65919",
          "name": "Pinecone API Key"
        }
      },
      "notes": "PATCH 5.5: Generate hypothetical questions",
      "retryOnFail": true,
      "maxTries": 3,
      "waitBetweenTries": 2000
    },
    {
      "parameters": {
        "jsCode": "// Add Q&A to chunks metadata\nconst chunkData = $node['Chunk Enricher V3.1 (Contextual)'].json;\nlet qaData;\ntry {\n  qaData = JSON.parse($json.choices?.[0]?.message?.content || '{}');\n} catch (e) {\n  qaData = { questions: [] };\n}\n\nconst questions = qaData.questions || [];\n\n// Add hypothetical questions to each chunk\nconst finalChunks = chunkData.chunks.map((chunk, idx) => ({\n  ...chunk,\n  hypothetical_questions: questions.slice(idx * 3, (idx + 1) * 3)\n}));\n\nreturn {\n  ...chunkData,\n  chunks: finalChunks\n};"
      },
      "id": "fe943522-76c8-41a6-b0e3-40e579772905",
      "name": "Q&A Enricher",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -752,
        112
      ]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "={{ $vars.EMBEDDING_API_URL || 'https://api.openai.com/v1/embeddings' }}",
        "authentication": "genericCredentialType",
        "genericAuthType": "httpHeaderAuth",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={\n  \"model\": \"{{ $vars.EMBEDDING_MODEL || 'text-embedding-3-small' }}\",\n  \"input\": {{ JSON.stringify(($json.chunks || []).slice(0, 100).map(c => c.contextual_content || c.content)) }}\n}",
        "options": {
          "timeout": 30000
        }
      },
      "id": "8e2ee231-f212-496e-b5e0-eaa0cfad0a8b",
      "name": "Generate Embeddings V3.1 (Contextual)",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.3,
      "position": [
        -496,
        112
      ],
      "credentials": {
        "httpHeaderAuth": {
          "id": "3DEiHDwB09D65919",
          "name": "Pinecone API Key"
        }
      },
      "retryOnFail": true,
      "maxTries": 3,
      "waitBetweenTries": 2000
    },
    {
      "parameters": {
        "jsCode": "// Prepare Vectors V3.1 - Uses contextual_content for embeddings\nconst chunkData = $node['Q&A Enricher'].json;\nconst embeddings = $json.data || [];\nconst chunks = chunkData.chunks || [];\n\nconst vectors = chunks.slice(0, 100).map((chunk, idx) => {\n  const embedding = embeddings[idx]?.embedding || [];\n  \n  return {\n    id: chunk.id,\n    values: embedding,\n    metadata: {\n      // V3.1: Store both original and contextual\n      content: chunk.content.substring(0, 1000),\n      contextual_content: (chunk.contextual_content || chunk.content).substring(0, 1500),\n      contextual_prefix: chunk.contextual_prefix || '',\n      \n      // Document metadata\n      parent_id: chunk.parent_id,\n      parent_filename: chunk.parent_filename,\n      document_title: chunk.document_title || '',\n      document_type: chunk.document_type || '',\n      section: chunk.section || '',\n      \n      // Chunk metadata\n      chunk_index: chunk.chunk_index,\n      total_chunks: chunk.total_chunks,\n      topic: chunk.topic || 'general',\n      \n      // Q&A enhancement\n      hypothetical_questions: chunk.hypothetical_questions || [],\n      \n      // ACL & tenant\n      tenant_id: chunk.tenant_id || 'default',\n      allowed_groups: ['default', 'admin'],\n      \n      // Quality\n      quality_score: chunk.quality_score || 0.5,\n      \n      // Timestamps\n      ingested_at: new Date().toISOString(),\n      version: chunk.version || 1\n    }\n  };\n});\n\nreturn {\n  vectors: vectors,\n  trace_id: chunkData.trace_id,\n  chunk_count: vectors.length,\n  contextual_enabled: true\n};"
      },
      "id": "cca4ed4b-00e6-4c87-b88e-2b971db78027",
      "name": "Prepare Vectors V3.1 (Contextual)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -256,
        112
      ]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "={{ $vars.PINECONE_URL }}/vectors/upsert",
        "authentication": "genericCredentialType",
        "genericAuthType": "httpHeaderAuth",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={\n  \"vectors\": {{ JSON.stringify($json.vectors) }}\n}",
        "options": {
          "timeout": 30000
        }
      },
      "id": "b0534017-1ec7-44e6-90d8-3ae6f505f163",
      "name": "Pinecone Upsert",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.3,
      "position": [
        0,
        0
      ],
      "credentials": {
        "httpHeaderAuth": {
          "id": "3DEiHDwB09D65919",
          "name": "Pinecone API Key"
        }
      },
      "onError": "continueErrorOutput"
    },
    {
      "parameters": {
        "schema": "public",
        "table": "documents",
        "columns": {
          "0": "i",
          "1": "d",
          "2": ",",
          "3": " ",
          "4": "c",
          "5": "o",
          "6": "n",
          "7": "t",
          "8": "e",
          "9": "n",
          "10": "t",
          "11": ",",
          "12": " ",
          "13": "p",
          "14": "a",
          "15": "r",
          "16": "e",
          "17": "n",
          "18": "t",
          "19": "_",
          "20": "i",
          "21": "d",
          "22": ",",
          "23": " ",
          "24": "p",
          "25": "a",
          "26": "r",
          "27": "e",
          "28": "n",
          "29": "t",
          "30": "_",
          "31": "f",
          "32": "i",
          "33": "l",
          "34": "e",
          "35": "n",
          "36": "a",
          "37": "m",
          "38": "e",
          "39": ",",
          "40": " ",
          "41": "s",
          "42": "u",
          "43": "m",
          "44": "m",
          "45": "a",
          "46": "r",
          "47": "y",
          "48": "_",
          "49": "c",
          "50": "o",
          "51": "n",
          "52": "t",
          "53": "e",
          "54": "x",
          "55": "t",
          "56": ",",
          "57": " ",
          "58": "q",
          "59": "u",
          "60": "a",
          "61": "l",
          "62": "i",
          "63": "t",
          "64": "y",
          "65": "_",
          "66": "s",
          "67": "c",
          "68": "o",
          "69": "r",
          "70": "e",
          "71": ",",
          "72": " ",
          "73": "v",
          "74": "e",
          "75": "r",
          "76": "s",
          "77": "i",
          "78": "o",
          "79": "n",
          "80": ",",
          "81": " ",
          "82": "i",
          "83": "s",
          "84": "_",
          "85": "o",
          "86": "b",
          "87": "s",
          "88": "o",
          "89": "l",
          "90": "e",
          "91": "t",
          "92": "e",
          "93": ",",
          "94": " ",
          "95": "c",
          "96": "h",
          "97": "u",
          "98": "n",
          "99": "k",
          "100": "_",
          "101": "m",
          "102": "e",
          "103": "t",
          "104": "h",
          "105": "o",
          "106": "d",
          "107": ",",
          "108": " ",
          "109": "t",
          "110": "e",
          "111": "n",
          "112": "a",
          "113": "n",
          "114": "t",
          "115": "_",
          "116": "i",
          "117": "d",
          "118": ",",
          "119": " ",
          "120": "c",
          "121": "r",
          "122": "e",
          "123": "a",
          "124": "t",
          "125": "e",
          "126": "d",
          "127": "_",
          "128": "a",
          "129": "t",
          "mappingMode": "defineBelow",
          "value": {},
          "matchingColumns": [],
          "schema": [],
          "attemptToConvertTypes": false,
          "convertFieldsToString": false
        },
        "options": {}
      },
      "id": "c96d5adc-06d8-41fb-bcae-33594ad9bc2c",
      "name": "Postgres Store",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.4,
      "position": [
        0,
        208
      ],
      "credentials": {
        "postgres": {
          "id": "0bf5AHN9S8qJTBr8",
          "name": "Postgres account"
        }
      },
      "onError": "continueErrorOutput"
    },
    {
      "parameters": {
        "jsCode": "// Prepare lock release\nconst vectorData = $node['Prepare Vectors V3.1 (Contextual)'].json;\n\nreturn {\n  lockKey: vectorData.lock_key,\n  lockValue: vectorData.lock_value,\n  trace_id: vectorData.trace_id\n};"
      },
      "id": "5399caac-ef0c-4dfe-a2a6-4220b2027391",
      "name": "Prepare Lock Release",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        256,
        112
      ]
    },
    {
      "parameters": {
        "operation": "delete",
        "key": "=={{ $json.lockKey }}"
      },
      "id": "40723c1d-6da4-42c5-915a-dc5ed7358092",
      "name": "Redis: Release Lock",
      "type": "n8n-nodes-base.redis",
      "typeVersion": 1,
      "position": [
        464,
        112
      ],
      "credentials": {
        "redis": {
          "id": "O2KEPiv7VzgDG5ZX",
          "name": "Redis Upstash"
        }
      }
    },
    {
      "parameters": {
        "method": "POST",
        "url": "={{ $vars.OTEL_COLLECTOR_URL || 'https://otel-collector.internal' }}/export",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={\n  \"trace_id\": \"{{ $json.trace_id }}\",\n  \"span_name\": \"INGESTION_V3_COMPLETE\",\n  \"status\": \"SUCCESS\"\n}",
        "options": {
          "timeout": 5000
        }
      },
      "id": "62b3572c-9b6f-4085-bcab-71c4fb81f3c9",
      "name": "Export Trace OTEL",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.3,
      "position": [
        656,
        112
      ]
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ { status: 'SKIPPED', reason: 'DUPLICATE_DOCUMENT', trace_id: $json.traceId } }}",
        "options": {}
      },
      "id": "780f8d18-42b8-4f41-af07-6a510dc9d76b",
      "name": "Return Skip Response",
      "type": "n8n-nodes-base.respondToWebhook",
      "position": [
        -2496,
        304
      ],
      "typeVersion": 1.1
    },
    {
      "parameters": {},
      "id": "b55f47b3-b62a-43eb-a306-e3a116f94783",
      "name": "Error Handler",
      "type": "n8n-nodes-base.errorTrigger",
      "typeVersion": 1,
      "position": [
        -3552,
        512
      ]
    },
    {
      "parameters": {
        "content": "# ðŸ”§ VARIABLES D'ENVIRONNEMENT REQUISES (SOTA 2026)\n\n## LLM APIs\n- `DEEPSEEK_API_KEY` - API key DeepSeek (api.deepseek.com)\n- `ANTHROPIC_API_KEY` - API key Anthropic (pour planning)\n- `GOOGLE_API_KEY` - API key Google (Gemini Flash)\n\n## LLM URLs (optionnel - dÃ©fauts inclus)\n- `SQL_GENERATION_API_URL` - URL API pour SQL\n- `INTENT_ANALYSIS_API_URL` - URL API pour intent\n- `PLANNING_API_URL` - URL API pour planning\n- `GENERATION_API_URL` - URL API pour gÃ©nÃ©ration\n\n## LLM Models (optionnel)\n- `SQL_GENERATION_MODEL` = deepseek-chat\n- `INTENT_ANALYSIS_MODEL` = deepseek-chat\n- `PLANNING_MODEL` = claude-sonnet-4-5-20250929\n- `GENERATION_MODEL` = deepseek-chat\n\n## Embedding (RECOMMANDÃ‰: self-hosted)\n- `EMBEDDING_API_URL` - URL embedding (dÃ©faut: OpenAI)\n- `EMBEDDING_MODEL` - text-embedding-3-small â†’ Qwen3-Embedding-8B\n\n## Reranking (RECOMMANDÃ‰: self-hosted)\n- `RERANKER_API_URL` - URL reranker (dÃ©faut: Cohere)\n- `RERANKER_MODEL` - rerank-v3 â†’ Qwen3-Reranker-8B\n\n## Impact estimÃ© (selon research):\n- -89% coÃ»t ingestion\n- -95% coÃ»t requÃªtes\n- +3% accuracy SQL (BIRD-SQL)\n- +15 pts MTEB (embedding)\n",
        "height": 600,
        "width": 400
      },
      "id": "env-vars-note",
      "name": "ðŸ“‹ Configuration SOTA 2026",
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        -4048,
        0
      ]
    },
    {
      "parameters": {
        "jsCode": "// P01: Prepare Contextual Prompts\n// Splits chunks into individual items for SplitInBatches processing\n\nconst chunks = $json.chunks || [];\nconst fullDocument = $json.full_document_text || '';\nconst documentTitle = $json.document_title || '';\n\nconst API_URL = '{{ $vars.CONTEXTUAL_RETRIEVAL_API_URL || \"https://api.deepseek.com/v1/chat/completions\" }}';\nconst MODEL = '{{ $vars.CONTEXTUAL_RETRIEVAL_MODEL || \"deepseek-chat\" }}';\n\nconst MAX_DOC_CHARS = 60000;\nlet docContext = fullDocument;\nif (docContext.length > MAX_DOC_CHARS) {\n  const half = Math.floor(MAX_DOC_CHARS / 2);\n  docContext = docContext.substring(0, half) + '\\n[...]\\n' + docContext.substring(docContext.length - half);\n}\n\nconst items = chunks.map((chunk, idx) => ({\n  json: {\n    ...chunk,\n    _contextual_prompt: JSON.stringify({\n      model: MODEL,\n      messages: [\n        {\n          role: 'system',\n          content: 'Tu recois un document complet et un chunk. Genere 1-2 phrases qui situent ce chunk dans le contexte du document. Reponds UNIQUEMENT avec les phrases de contexte.'\n        },\n        {\n          role: 'user',\n          content: `<document>\\n${docContext}\\n</document>\\n<chunk>\\n${chunk.content}\\n</chunk>\\nSitue ce chunk dans le contexte du document \"${documentTitle}\".`\n        }\n      ],\n      temperature: 0.0,\n      max_tokens: 200\n    }),\n    _chunk_index: idx,\n    _parent_data: {\n      parent_id: $json.parent_id,\n      parent_filename: $json.parent_filename,\n      document_title: $json.document_title,\n      total_chunks: chunks.length,\n      trace_id: $json.trace_id,\n      tenant_id: $json.tenant_id,\n      lock_key: $json.lock_key,\n      lock_value: $json.lock_value\n    }\n  }\n}));\n\nreturn items;"
      },
      "id": "contextual-retrieval-prep",
      "name": "Prepare Contextual Prompts",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -1400,
        112
      ]
    },
    {
      "parameters": {
        "batchSize": 5,
        "options": {}
      },
      "id": "contextual-retrieval-split",
      "name": "Split Chunks for Context",
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 3,
      "position": [
        -1350,
        112
      ]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "={{ $vars.CONTEXTUAL_RETRIEVAL_API_URL || 'https://api.deepseek.com/v1/chat/completions' }}",
        "authentication": "genericCredentialType",
        "genericAuthType": "httpHeaderAuth",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ $json._contextual_prompt }}",
        "options": {
          "timeout": 30000
        }
      },
      "id": "contextual-retrieval-llm",
      "name": "Contextual LLM Call",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.3,
      "position": [
        -1300,
        112
      ],
      "retryOnFail": true,
      "maxTries": 3,
      "waitBetweenTries": 1500,
      "credentials": {
        "httpHeaderAuth": {
          "id": "CREDENTIAL_ID_DEEPSEEK",
          "name": "DeepSeek API Key"
        }
      },
      "notes": "P01: Per-chunk contextual retrieval LLM call"
    },
    {
      "parameters": {
        "jsCode": "// P01: Aggregate Contextual Chunks\n// Collects all contextually-enriched chunks back together\n\nconst allItems = $input.all();\nconst enrichedChunks = [];\nlet parentData = null;\n\nfor (const item of allItems) {\n  const chunk = item.json;\n  const contextualHeader = chunk.choices?.[0]?.message?.content?.trim() || '';\n\n  // Recover original chunk data\n  const originalChunk = {\n    id: chunk.id || chunk._chunk_id,\n    content: chunk.content || chunk._original_content,\n    topic: chunk.topic,\n    section: chunk.section,\n    parent_id: chunk.parent_id,\n    parent_filename: chunk.parent_filename,\n    document_title: chunk.document_title,\n    document_type: chunk.document_type,\n    quality_score: chunk.quality_score,\n    version: chunk.version,\n    is_obsolete: chunk.is_obsolete,\n    chunk_method: chunk.chunk_method,\n    chunk_index: chunk._chunk_index || chunk.chunk_index,\n    total_chunks: chunk.total_chunks,\n    tenant_id: chunk.tenant_id,\n    trace_id: chunk.trace_id,\n    pii_count: chunk.pii_count,\n    created_at: chunk.created_at\n  };\n\n  // Apply contextual header\n  originalChunk.contextual_header = contextualHeader;\n  originalChunk.contextual_content = contextualHeader\n    ? `${contextualHeader}\\n\\n${originalChunk.content}`\n    : originalChunk.content;\n  originalChunk.contextual_prefix = contextualHeader;\n  originalChunk.contextual_retrieval_applied = !!contextualHeader;\n\n  enrichedChunks.push(originalChunk);\n\n  if (!parentData && chunk._parent_data) {\n    parentData = chunk._parent_data;\n  }\n}\n\nconst successCount = enrichedChunks.filter(c => c.contextual_retrieval_applied).length;\n\nreturn {\n  chunks: enrichedChunks,\n  parent_id: parentData?.parent_id,\n  parent_filename: parentData?.parent_filename,\n  document_title: parentData?.document_title,\n  total_chunks: enrichedChunks.length,\n  trace_id: parentData?.trace_id,\n  tenant_id: parentData?.tenant_id,\n  lock_key: parentData?.lock_key,\n  lock_value: parentData?.lock_value,\n  contextual_retrieval_stats: {\n    total: enrichedChunks.length,\n    success: successCount,\n    failed: enrichedChunks.length - successCount\n  }\n};"
      },
      "id": "contextual-retrieval-agg",
      "name": "Aggregate Contextual Chunks",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -1250,
        112
      ],
      "notes": "P01: Reassembles contextually-enriched chunks"
    },
    {
      "parameters": {
        "jsCode": "// PATCH P02: BM25 Sparse Vector Generator for Pinecone Hybrid Search\n// Impact: +30-50% recall combined with dense embeddings\n// Uses client-side BM25 scoring with hashed sparse vectors\n\nconst chunks = $json.chunks || [];\n\n// === BM25 PARAMETERS ===\nconst K1 = 1.2;\nconst B = 0.75;\n\n// === Tokenizer (FR+EN) ===\nconst STOP_WORDS = new Set([\n  'le', 'la', 'les', 'un', 'une', 'des', 'de', 'du', 'au', 'aux',\n  'et', 'ou', 'mais', 'donc', 'car', 'ni', 'que', 'qui', 'quoi',\n  'ce', 'cette', 'ces', 'mon', 'ton', 'son', 'ma', 'ta', 'sa',\n  'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to',\n  'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be',\n  'it', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she',\n  'nous', 'vous', 'ils', 'elles', 'je', 'tu', 'il', 'elle',\n  'est', 'sont', 'a', 'ont', 'pas', 'ne', 'plus', 'comme'\n]);\n\nfunction tokenize(text) {\n  return text\n    .toLowerCase()\n    .normalize('NFD').replace(/[\\u0300-\\u036f]/g, '')\n    .replace(/[^a-z0-9\\s]/g, ' ')\n    .split(/\\s+/)\n    .filter(t => t.length > 2 && !STOP_WORDS.has(t));\n}\n\nfunction hashToken(token) {\n  let hash = 0;\n  for (let i = 0; i < token.length; i++) {\n    const char = token.charCodeAt(i);\n    hash = ((hash << 5) - hash) + char;\n    hash = hash & hash;\n  }\n  return Math.abs(hash) % 100000;\n}\n\n// === Document Frequencies ===\nconst allTokenSets = chunks.map(chunk => {\n  const text = chunk.contextual_content || chunk.content;\n  return new Set(tokenize(text));\n});\n\nconst df = {};\nfor (const tokenSet of allTokenSets) {\n  for (const token of tokenSet) {\n    df[token] = (df[token] || 0) + 1;\n  }\n}\n\nconst N = chunks.length;\nconst avgDl = chunks.reduce((sum, c) => {\n  return sum + tokenize(c.contextual_content || c.content).length;\n}, 0) / Math.max(N, 1);\n\n// === BM25 Sparse Vectors ===\nconst sparseVectors = chunks.map((chunk) => {\n  const text = chunk.contextual_content || chunk.content;\n  const tokens = tokenize(text);\n  const dl = tokens.length;\n\n  const tf = {};\n  for (const token of tokens) {\n    tf[token] = (tf[token] || 0) + 1;\n  }\n\n  const indices = [];\n  const values = [];\n\n  for (const [term, freq] of Object.entries(tf)) {\n    const idf = Math.log((N - (df[term] || 0) + 0.5) / ((df[term] || 0) + 0.5) + 1);\n    const tfNorm = (freq * (K1 + 1)) / (freq + K1 * (1 - B + B * (dl / avgDl)));\n    const score = idf * tfNorm;\n\n    if (score > 0) {\n      indices.push(hashToken(term));\n      values.push(Math.round(score * 1000) / 1000);\n    }\n  }\n\n  return { indices, values };\n});\n\nconst enrichedChunks = chunks.map((chunk, idx) => ({\n  ...chunk,\n  sparse_values: sparseVectors[idx],\n  bm25_token_count: tokenize(chunk.contextual_content || chunk.content).length\n}));\n\nreturn {\n  ...$json,\n  chunks: enrichedChunks,\n  bm25_stats: {\n    vocabulary_size: Object.keys(df).length,\n    avg_doc_length: Math.round(avgDl),\n    total_chunks: N\n  }\n};"
      },
      "id": "bm25-sparse-gen",
      "name": "BM25 Sparse Vector Generator",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -376,
        112
      ],
      "notes": "PATCH P02: BM25 sparse vectors for Pinecone hybrid search"
    }
  ],
  "pinData": {},
  "connections": {
    "S3 Event Webhook": {
      "main": [
        [
          {
            "node": "Init Lock & Trace",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Init Lock & Trace": {
      "main": [
        [
          {
            "node": "Redis: Acquire Lock",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Redis: Acquire Lock": {
      "main": [
        [
          {
            "node": "Lock Result Handler",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Lock Result Handler": {
      "main": [
        [
          {
            "node": "Lock Acquired?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Lock Acquired?": {
      "main": [
        [
          {
            "node": "MIME Type Detector",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Return Skip Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "MIME Type Detector": {
      "main": [
        [
          {
            "node": "OCR Extraction",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "OCR Extraction": {
      "main": [
        [
          {
            "node": "PII Fortress",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "PII Fortress": {
      "main": [
        [
          {
            "node": "Semantic Chunker V3.1 (Adaptive)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Semantic Chunker V3.1 (Adaptive)": {
      "main": [
        [
          {
            "node": "Chunk Validator & Enricher V4",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Version Manager": {
      "main": [
        [
          {
            "node": "Q&A Generator",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Q&A Generator": {
      "main": [
        [
          {
            "node": "Q&A Enricher",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Q&A Enricher": {
      "main": [
        [
          {
            "node": "Generate Embeddings V3.1 (Contextual)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Generate Embeddings V3.1 (Contextual)": {
      "main": [
        [
          {
            "node": "BM25 Sparse Vector Generator",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Vectors V3.1 (Contextual)": {
      "main": [
        [
          {
            "node": "Pinecone Upsert",
            "type": "main",
            "index": 0
          },
          {
            "node": "Postgres Store",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Pinecone Upsert": {
      "main": [
        [
          {
            "node": "Prepare Lock Release",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Postgres Store": {
      "main": [
        [
          {
            "node": "Prepare Lock Release",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Lock Release": {
      "main": [
        [
          {
            "node": "Redis: Release Lock",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Redis: Release Lock": {
      "main": [
        [
          {
            "node": "Export Trace OTEL",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Chunk Validator & Enricher V4": {
      "main": [
        [
          {
            "node": "Prepare Contextual Prompts",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Contextual Prompts": {
      "main": [
        [
          {
            "node": "Split Chunks for Context",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Split Chunks for Context": {
      "main": [
        [
          {
            "node": "Contextual LLM Call",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Aggregate Contextual Chunks",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Contextual LLM Call": {
      "main": [
        [
          {
            "node": "Split Chunks for Context",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Aggregate Contextual Chunks": {
      "main": [
        [
          {
            "node": "Version Manager",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "BM25 Sparse Vector Generator": {
      "main": [
        [
          {
            "node": "Prepare Vectors V3.1 (Contextual)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": true,
  "settings": {
    "executionOrder": "v1",
    "callerPolicy": "workflowsFromSameOwner",
    "availableInMCP": true
  },
  "versionId": "750f530d-17c1-406a-b0d3-28a256dab657",
  "meta": {
    "instanceId": "810d143e45edb75891ee3244decd00dc613435a73f5b3ad2900fe9bc764e9d73"
  },
  "id": "nh1D4Up0wBZhuQbp",
  "tags": []
}