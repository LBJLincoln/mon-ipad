name: RAG Evaluation Pipeline

on:
  workflow_dispatch:
    inputs:
      dataset:
        description: 'Dataset to evaluate'
        default: 'phase-1'
        type: choice
        options:
          - phase-1
          - phase-2
          - all
      types:
        description: 'Pipeline types to test (comma-separated)'
        default: 'standard,graph,quantitative,orchestrator'
        type: string
      max_per_type:
        description: 'Max questions per pipeline (empty = all)'
        default: ''
        type: string
      reset:
        description: 'Reset dedup (re-test all questions)'
        default: false
        type: boolean
      label:
        description: 'Label for this evaluation run'
        default: ''
        type: string
  schedule:
    # Run daily at 06:00 UTC
    - cron: '0 6 * * *'

env:
  SUPABASE_PASSWORD: ${{ secrets.SUPABASE_PASSWORD }}
  PINECONE_API_KEY: ${{ secrets.PINECONE_API_KEY }}
  PINECONE_HOST: ${{ secrets.PINECONE_HOST }}
  NEO4J_PASSWORD: ${{ secrets.NEO4J_PASSWORD }}
  OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
  N8N_API_KEY: ${{ secrets.N8N_API_KEY }}
  N8N_HOST: https://amoret.app.n8n.cloud

jobs:
  evaluate:
    runs-on: ubuntu-latest
    timeout-minutes: 120

    steps:
      - uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Check phase gates
        if: ${{ inputs.dataset != 'phase-1' && inputs.dataset != '' }}
        run: |
          python3 -c "
          import json, sys
          with open('db/readiness/phase-1.json') as f:
              p1 = json.load(f)
          gates = p1.get('gate_criteria', {})
          unmet = [(k, v) for k, v in gates.items() if not v.get('met', False)]
          if unmet:
              print('PHASE GATE CHECK FAILED')
              for k, v in unmet:
                  print(f'  {k}: {v.get(\"current\",\"?\")}% (target: {v.get(\"target_accuracy\", v.get(\"target\",\"?\"))}%)')
              sys.exit(1)
          print('Phase 1 gates: ALL MET')
          "

      - name: Run RAG Evaluation
        run: |
          cd eval
          DATASET="${{ inputs.dataset || 'phase-1' }}"
          ARGS="--dataset $DATASET --force-phase"
          ARGS="$ARGS --types ${{ inputs.types || 'standard,graph,quantitative,orchestrator' }}"
          if [ -n "${{ inputs.max_per_type }}" ]; then
            ARGS="$ARGS --max ${{ inputs.max_per_type }}"
          fi
          if [ "${{ inputs.reset }}" = "true" ]; then
            ARGS="$ARGS --reset"
          fi
          if [ -n "${{ inputs.label }}" ]; then
            ARGS="$ARGS --label \"${{ inputs.label }}\""
          fi
          python run-eval-parallel.py $ARGS

      - name: Commit and push results
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add docs/data.json docs/tested-questions.json logs/
          git diff --staged --quiet || (git commit -m "eval: automated pipeline evaluation results" && git push)

      - name: Generate summary
        if: always()
        run: |
          python3 -c "
          import json
          with open('docs/data.json') as f:
              data = json.load(f)
          print('## RAG Evaluation Results')
          print()
          print('| Pipeline | Tested | Correct | Accuracy | Errors | Avg Latency |')
          print('|----------|--------|---------|----------|--------|-------------|')
          for name, p in data['pipelines'].items():
              tested = p.get('tested', 0)
              correct = p.get('correct', 0)
              acc = f\"{correct/tested*100:.1f}%\" if tested > 0 else 'N/A'
              print(f\"| {name} | {tested} | {correct} | {acc} | {p.get('errors',0)} | {p.get('avg_latency_ms',0)}ms |\")
          total_tested = sum(p.get('tested',0) for p in data['pipelines'].values())
          total_correct = sum(p.get('correct',0) for p in data['pipelines'].values())
          overall = f\"{total_correct/total_tested*100:.1f}%\" if total_tested > 0 else 'N/A'
          print(f\"| **Overall** | **{total_tested}** | **{total_correct}** | **{overall}** | | |')
          " >> $GITHUB_STEP_SUMMARY
