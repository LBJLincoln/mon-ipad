{
  "title": "AUDIT COMPLET — Datasets x Neo4j x Pinecone x Supabase",
  "generated_at": "2026-02-06",
  "auditor": "Claude Code (automated analysis)",

  "executive_summary": {
    "verdict": "RIEN N'EST PRET — Les 3 bases de données sont vides ou inaccessibles",
    "neo4j_status": "VIDE — Aucune entité/relation de graph RAG ingérée",
    "pinecone_status": "PSEUDO-VECTEURS UNIQUEMENT — Pas de vrais embeddings",
    "supabase_status": "QUESTIONS SEULEMENT — Pas de données tabulaires pour Quantitative RAG",
    "budget_2_dollars_embeddings": "LARGEMENT SUFFISANT — Coût estimé ~$0.06-$0.30",
    "cost_1000_questions_llm": "~$50 (Graph $40 + Quant $10) — HORS BUDGET $2",
    "blocker_count": 7
  },

  "question_1_datasets_in_databases": {
    "summary": "Les datasets EXISTENT dans les fichiers JSON locaux mais ne sont PAS dans Neo4j ni correctement dans Pinecone",

    "neo4j": {
      "url_configured": "https://38c949a2.databases.neo4j.io/db/neo4j/query/v2",
      "url_inconsistency": "agents-config.yaml utilise a9a062c3 vs claude-mcp-config.json utilise 38c949a2",
      "last_verification": "2026-02-06T20:23:41 — DNS resolution failed",
      "entity_graph_present": false,
      "what_should_be_there": [
        "Entity nodes (Organization, Person, City, etc.) extraits des contextes musique/2wikimultihopqa",
        "Relations typées (A_CREE, CONNECTE, CAUSE_PAR, etc.) avec poids et confidence",
        "Community detection (Louvain) avec community summaries",
        "Tenant isolation via tenant_id='benchmark'"
      ],
      "what_is_actually_there": "INCONNU — DNS resolution échoue depuis ce sandbox. Le script ingest-datasets.py tente de créer des BenchmarkQuestion nodes, mais PAS les Entity/Relationship nodes nécessaires au Graph RAG",
      "critical_gap": "Le script d'ingestion crée des noeuds BenchmarkQuestion (question+réponse) mais le workflow Graph RAG V3.3 attend des Entity nodes avec des relations traversables sur 3 hops. Ce sont deux choses COMPLETEMENT différentes.",
      "datasets_that_need_neo4j": {
        "musique": {"questions": 200, "needs": "Entity graph from paragraph contexts"},
        "2wikimultihopqa": {"questions": 300, "needs": "Entity graph from supporting facts"},
        "hotpotqa": {"questions": 1000, "needs": "Entity graph from contexts (for standard RAG 28K)"}
      }
    },

    "pinecone": {
      "host_configured": "https://n8nultimate-a4mkzmz.svc.aped-4627-b74a.pinecone.io",
      "alternative_host": "https://sota-rag-a4mkzmz.svc.aped-4627-b74a.pinecone.io (in some scripts)",
      "last_verification": "2026-02-06T20:23:41 — DNS resolution failed",
      "real_embeddings_present": false,
      "pseudo_vectors_present": "PROBABLE — ingest-datasets.py creates SHA256-hash-based 1536-dim pseudo-vectors",
      "what_should_be_there": "Real text-embedding-3-small vectors (1536-dim) of context passages for similarity search",
      "what_is_actually_there": "Pseudo-vectors basés sur SHA256 du texte de la question — INUTILISABLES pour la recherche sémantique",
      "critical_gap": "Les pseudo-vecteurs retourneront des résultats ALÉATOIRES car ils ne capturent aucune sémantique",
      "namespaces_expected": [
        "benchmark-musique",
        "benchmark-2wikimultihopqa",
        "benchmark-hotpotqa",
        "benchmark-finqa",
        "benchmark-tatqa",
        "benchmark-convfinqa",
        "benchmark-wikitablequestions",
        "benchmark-triviaqa",
        "benchmark-squad_v2",
        "benchmark-popqa",
        "benchmark-pubmedqa",
        "benchmark-frames"
      ],
      "dimension": 1536,
      "embedding_model_needed": "openai/text-embedding-3-small via OpenRouter"
    },

    "supabase": {
      "host": "db.ayqviqmxifzmhphiqfmj.supabase.co",
      "last_verification": "2026-02-06T20:23:41 — DNS resolution failed",
      "questions_in_supabase": 28283,
      "answers_received": 69,
      "errors": 28134,
      "tabular_data_for_quantitative": false,
      "what_should_be_there_for_quant": [
        "finqa: Financial reports tables (income statements, balance sheets)",
        "tatqa: Hybrid table+text financial documents",
        "convfinqa: Conversational financial tables",
        "wikitablequestions: Wikipedia infobox tables"
      ],
      "what_is_actually_there": "benchmark_datasets table avec questions/réponses, mais PAS de tables financières/tabulaires pour Text-to-SQL",
      "critical_gap": "Le Quantitative RAG fait du Text-to-SQL. Sans tables SQL avec les données financières, il retourne NULL systématiquement."
    }
  },

  "question_2_embedding_budget": {
    "model": "openai/text-embedding-3-small",
    "api_url": "https://openrouter.ai/api/v1/embeddings",
    "pricing": "$0.02 per 1M tokens (OpenRouter = même prix qu'OpenAI direct)",
    "dimension": 1536,
    "budget_available": "$2.00",

    "cost_estimates": {
      "scenario_1_questions_only": {
        "description": "Embedder uniquement les 1000 questions (minimal, inutile seul)",
        "tokens": "~50,000 (1000 questions × ~50 tokens/question)",
        "cost": "$0.001",
        "verdict": "Négligeable"
      },
      "scenario_2_questions_plus_contexts_1000": {
        "description": "Embedder questions + contextes pour les 1000 questions Graph+Quant",
        "tokens": "~2,500,000 (500 graph × 5 passages × 800 tokens + 500 quant × 500 tokens)",
        "cost": "$0.05",
        "verdict": "Largement dans le budget"
      },
      "scenario_3_all_benchmark_datasets": {
        "description": "Embedder TOUS les passages de contexte des 12 datasets (8,824 items avec contextes)",
        "tokens": "~5,000,000-15,000,000",
        "cost": "$0.10-$0.30",
        "verdict": "Toujours dans le budget de $2"
      },
      "scenario_4_full_corpus_with_contextual_retrieval": {
        "description": "Contextual Retrieval (Anthropic method) = LLM call par chunk + embedding",
        "tokens_embedding": "~15,000,000",
        "cost_embedding": "$0.30",
        "cost_llm_contextual": "~$0.60 (15K chunks × $0.04/chunk avec DeepSeek-V3)",
        "total": "$0.90",
        "verdict": "Encore dans le budget"
      }
    },

    "conclusion": "$2 est LARGEMENT suffisant pour les embeddings. Même le scénario le plus coûteux (full corpus + contextual retrieval) coûte < $1. Le vrai coût est le LLM inference pendant les tests RAG."
  },

  "question_3_what_is_missing_before_1000_test": {
    "blockers_critiques": [
      {
        "id": "B1",
        "severity": "CRITICAL",
        "title": "Neo4j vide — Pas de graph d'entités",
        "description": "Le workflow Graph RAG V3.3 fait des traversées Cypher sur 3 hops entre Entity nodes. Ces entités n'existent pas. Il faut lancer le pipeline d'enrichissement (Enrichissement V3.1) sur les contextes de musique et 2wikimultihopqa pour extraire entités + relations.",
        "effort": "Lourd — nécessite un pipeline LLM d'extraction d'entités",
        "workaround": "Aucun — sans données dans Neo4j, les 500 questions Graph RAG échoueront à 100%"
      },
      {
        "id": "B2",
        "severity": "CRITICAL",
        "title": "Pinecone contient des pseudo-vecteurs, pas de vrais embeddings",
        "description": "Le script ingest-datasets.py a créé des vecteurs basés sur SHA256 hashes, pas des embeddings sémantiques. La recherche par similarité ne fonctionnera pas.",
        "effort": "Moyen — script à modifier pour appeler l'API OpenRouter embeddings",
        "cost": "~$0.05 pour 1000 questions+contextes",
        "workaround": "Modifier ingest-datasets.py pour utiliser l'API d'embeddings au lieu des pseudo-vecteurs"
      },
      {
        "id": "B3",
        "severity": "CRITICAL",
        "title": "Supabase n'a pas de tables financières pour Text-to-SQL",
        "description": "Les 500 questions quantitatives (finqa, tatqa, convfinqa, wikitablequestions) nécessitent des tables SQL avec des données financières réelles. Actuellement seule la table benchmark_datasets existe.",
        "effort": "Moyen — il faut parser les datasets HuggingFace et créer des tables SQL",
        "workaround": "Aucun pour Quantitative RAG"
      },
      {
        "id": "B4",
        "severity": "CRITICAL",
        "title": "Graph RAG retourne des community summaries bruts au lieu de réponses LLM",
        "description": "Même avec des données dans Neo4j, le workflow Graph RAG V3.3 a un bug d'extraction de réponse : il retourne les community summaries concaténés avec ' | ' au lieu de la réponse du LLM.",
        "workflow": "TEST - SOTA 2026 - WF2 Graph RAG V3.3 - CORRECTED (1).json",
        "effort": "Léger — corriger le noeud d'extraction de réponse"
      },
      {
        "id": "B5",
        "severity": "HIGH",
        "title": "Routing orchestrateur défaillant pour Graph RAG",
        "description": "Seulement 21.6% des questions graph sont correctement routées. 72.8% sont 'ambiguës'. Les questions multi-hop risquent d'aller vers Standard RAG au lieu de Graph RAG.",
        "effort": "Moyen — améliorer le prompt de routing dans l'orchestrateur"
      },
      {
        "id": "B6",
        "severity": "HIGH",
        "title": "Évaluation par exact-match au lieu de F1",
        "description": "L'évaluation actuelle compare les réponses en exact-match. Une réponse correcte mais verbose obtient un score de 0. Il faut implémenter le token-level F1 score.",
        "effort": "Léger"
      },
      {
        "id": "B7",
        "severity": "MEDIUM",
        "title": "Incohérence des URLs Neo4j",
        "description": "agents-config.yaml pointe vers a9a062c3.databases.neo4j.io mais claude-mcp-config.json et ingest-datasets.py pointent vers 38c949a2.databases.neo4j.io. L'URL que tu as fournie (38c949a2) est celle utilisée dans les scripts.",
        "effort": "Trivial — aligner les configs"
      }
    ],

    "ordre_de_resolution_recommande": [
      "1. B7 — Aligner les URLs Neo4j dans toutes les configs",
      "2. B2 — Remplacer les pseudo-vecteurs par de vrais embeddings ($0.05)",
      "3. B3 — Créer les tables SQL et ingérer les données tabulaires",
      "4. B1 — Lancer le pipeline d'enrichissement pour créer le graph d'entités",
      "5. B4 — Fixer l'extraction de réponse du Graph RAG",
      "6. B5 — Améliorer le routing de l'orchestrateur",
      "7. B6 — Implémenter F1 scoring au lieu d'exact-match"
    ]
  },

  "question_4_cost_1000_questions": {
    "embedding_cost": {
      "questions_plus_contexts": "$0.05",
      "model": "text-embedding-3-small via OpenRouter",
      "within_2_dollar_budget": true
    },
    "llm_inference_cost": {
      "graph_rag_500_questions": {
        "per_query": "$0.08 (Neo4j + Pinecone + Cohere Rerank + LLM)",
        "total": "$40"
      },
      "quantitative_rag_500_questions": {
        "per_query": "$0.02 (Postgres + LLM SQL)",
        "total": "$10"
      },
      "total_llm": "$50"
    },
    "reranking_cost": {
      "model": "Cohere rerank-v3.5",
      "per_query": "~$0.001",
      "total_1000": "~$1"
    },
    "total_estimated": {
      "embeddings": "$0.05",
      "llm_inference": "$50",
      "reranking": "$1",
      "grand_total": "~$51",
      "note": "Le $2 de budget couvre UNIQUEMENT les embeddings. Le LLM inference coûte ~$50 supplémentaires via les workflows n8n (Anthropic API pour les réponses RAG)."
    },
    "cost_optimization_tips": [
      "Utiliser Haiku au lieu d'Opus pour les questions simples (-60x le coût)",
      "Réduire le nombre de hops Neo4j de 3 à 2 (-30% latence)",
      "Batch les appels embedding (100 par requête) pour éviter l'overhead réseau",
      "Commencer par 50 questions de test avant les 1000 pour valider le pipeline"
    ]
  },

  "datasets_inventory": {
    "total_questions_in_files": 29053,
    "graph_rag_1000_questions_file": "benchmark-workflows/rag-1000-test-questions.json",
    "graph_rag_questions": {
      "musique": 200,
      "2wikimultihopqa": 300
    },
    "quantitative_rag_questions": {
      "finqa": 200,
      "tatqa": 150,
      "convfinqa": 100,
      "wikitablequestions": 50
    },
    "standard_rag_in_supabase": {
      "asqa": 2750,
      "finqa": 2830,
      "frames": 1943,
      "hotpotqa": 3320,
      "msmarco": 5250,
      "narrativeqa": 2750,
      "popqa": 2250,
      "pubmedqa": 1340,
      "squad_v2": 2600,
      "triviaqa": 3250
    }
  }
}
