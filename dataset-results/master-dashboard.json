{
  "title": "RAG Testing â€” Master Analysis Dashboard",
  "generated_at": "2026-02-06T20:23:14.870425",
  "overview": {
    "total_datasets": 16,
    "total_questions": 29053,
    "total_tested": 28053,
    "total_answered": 0,
    "total_errors": 28053,
    "overall_answer_rate": "0.0%",
    "overall_test_coverage": "96.6%"
  },
  "specialized_graph_rag": {
    "description": "Graph RAG questions requiring multi-hop reasoning (Neo4j + entity traversal)",
    "datasets": [
      "musique",
      "2wikimultihopqa"
    ],
    "total_questions": 500,
    "total_tested": 0,
    "total_answered": 0,
    "answer_rate": "0%",
    "details": [
      {
        "file": "results-2wikimultihopqa.json",
        "dataset_name": "2wikimultihopqa",
        "rag_target": "graph",
        "category": "multi_hop_qa",
        "total_questions": 300,
        "total_tested": 0,
        "total_answered": 0,
        "total_errors": 0,
        "avg_f1": 0.0,
        "answer_rate": "0%",
        "status": "NOT_STARTED",
        "data_ready": false
      },
      {
        "file": "results-musique.json",
        "dataset_name": "musique",
        "rag_target": "graph",
        "category": "multi_hop_qa",
        "total_questions": 200,
        "total_tested": 0,
        "total_answered": 0,
        "total_errors": 0,
        "avg_f1": 0.0,
        "answer_rate": "0%",
        "status": "NOT_STARTED",
        "data_ready": false
      }
    ],
    "known_issues": [
      "Graph RAG returns raw community summaries instead of LLM answers",
      "Neo4j data may not contain entities from musique/2wikimultihopqa",
      "Routing accuracy for graph questions is only ~21.6%"
    ]
  },
  "specialized_quantitative_rag": {
    "description": "Quantitative RAG questions requiring numerical/tabular reasoning (Text-to-SQL)",
    "datasets": [
      "finqa",
      "tatqa",
      "convfinqa",
      "wikitablequestions"
    ],
    "total_questions": 500,
    "total_tested": 0,
    "total_answered": 0,
    "answer_rate": "0%",
    "details": [
      {
        "file": "results-convfinqa.json",
        "dataset_name": "convfinqa",
        "rag_target": "quantitative",
        "category": "domain_finance",
        "total_questions": 100,
        "total_tested": 0,
        "total_answered": 0,
        "total_errors": 0,
        "avg_f1": 0.0,
        "answer_rate": "0%",
        "status": "NOT_STARTED",
        "data_ready": false
      },
      {
        "file": "results-finqa-quantitative.json",
        "dataset_name": "finqa",
        "rag_target": "quantitative",
        "category": "domain_finance",
        "total_questions": 200,
        "total_tested": 0,
        "total_answered": 0,
        "total_errors": 0,
        "avg_f1": 0.0,
        "answer_rate": "0%",
        "status": "NOT_STARTED",
        "data_ready": false
      },
      {
        "file": "results-tatqa.json",
        "dataset_name": "tatqa",
        "rag_target": "quantitative",
        "category": "domain_finance",
        "total_questions": 150,
        "total_tested": 0,
        "total_answered": 0,
        "total_errors": 0,
        "avg_f1": 0.0,
        "answer_rate": "0%",
        "status": "NOT_STARTED",
        "data_ready": false
      },
      {
        "file": "results-wikitablequestions.json",
        "dataset_name": "wikitablequestions",
        "rag_target": "quantitative",
        "category": "table_qa",
        "total_questions": 50,
        "total_tested": 0,
        "total_answered": 0,
        "total_errors": 0,
        "avg_f1": 0.0,
        "answer_rate": "0%",
        "status": "NOT_STARTED",
        "data_ready": false
      }
    ],
    "known_issues": [
      "Quantitative RAG needs tabular data ingested into SQL tables",
      "Text-to-SQL pipeline returns NULL when data is missing",
      "Table data from finqa/tatqa needs specific schema mapping"
    ]
  },
  "benchmark_standard_rag": {
    "description": "Standard RAG benchmark questions (originally all errored due to fetch() bug)",
    "datasets": [
      "asqa",
      "finqa",
      "frames",
      "hotpotqa",
      "msmarco",
      "narrativeqa",
      "popqa",
      "pubmedqa",
      "squad_v2",
      "triviaqa"
    ],
    "total_questions": 28053,
    "total_tested": 28053,
    "total_answered": 0,
    "answer_rate": "0.0%",
    "details": [
      {
        "file": "results-asqa.json",
        "dataset_name": "asqa",
        "rag_target": "standard",
        "category": "benchmark",
        "total_questions": 2750,
        "total_tested": 2750,
        "total_answered": 0,
        "total_errors": 2750,
        "avg_f1": 0.0,
        "answer_rate": "0.0%",
        "status": "ALL_ERRORED",
        "data_ready": null
      },
      {
        "file": "results-finqa.json",
        "dataset_name": "finqa",
        "rag_target": "standard",
        "category": "benchmark",
        "total_questions": 2830,
        "total_tested": 2830,
        "total_answered": 0,
        "total_errors": 2830,
        "avg_f1": 0.0,
        "answer_rate": "0.0%",
        "status": "ALL_ERRORED",
        "data_ready": null
      },
      {
        "file": "results-frames.json",
        "dataset_name": "frames",
        "rag_target": "standard",
        "category": "benchmark",
        "total_questions": 1943,
        "total_tested": 1943,
        "total_answered": 0,
        "total_errors": 1943,
        "avg_f1": 0.0,
        "answer_rate": "0.0%",
        "status": "ALL_ERRORED",
        "data_ready": null
      },
      {
        "file": "results-hotpotqa.json",
        "dataset_name": "hotpotqa",
        "rag_target": "standard",
        "category": "benchmark",
        "total_questions": 3320,
        "total_tested": 3320,
        "total_answered": 0,
        "total_errors": 3320,
        "avg_f1": 0.0,
        "answer_rate": "0.0%",
        "status": "ALL_ERRORED",
        "data_ready": null
      },
      {
        "file": "results-msmarco.json",
        "dataset_name": "msmarco",
        "rag_target": "standard",
        "category": "benchmark",
        "total_questions": 5250,
        "total_tested": 5250,
        "total_answered": 0,
        "total_errors": 5250,
        "avg_f1": 0.0,
        "answer_rate": "0.0%",
        "status": "ALL_ERRORED",
        "data_ready": null
      },
      {
        "file": "results-narrativeqa.json",
        "dataset_name": "narrativeqa",
        "rag_target": "standard",
        "category": "benchmark",
        "total_questions": 2750,
        "total_tested": 2750,
        "total_answered": 0,
        "total_errors": 2750,
        "avg_f1": 0.0,
        "answer_rate": "0.0%",
        "status": "ALL_ERRORED",
        "data_ready": null
      },
      {
        "file": "results-popqa.json",
        "dataset_name": "popqa",
        "rag_target": "standard",
        "category": "benchmark",
        "total_questions": 2250,
        "total_tested": 2250,
        "total_answered": 0,
        "total_errors": 2250,
        "avg_f1": 0.0,
        "answer_rate": "0.0%",
        "status": "ALL_ERRORED",
        "data_ready": null
      },
      {
        "file": "results-pubmedqa.json",
        "dataset_name": "pubmedqa",
        "rag_target": "standard",
        "category": "benchmark",
        "total_questions": 1340,
        "total_tested": 1340,
        "total_answered": 0,
        "total_errors": 1340,
        "avg_f1": 0.0,
        "answer_rate": "0.0%",
        "status": "ALL_ERRORED",
        "data_ready": null
      },
      {
        "file": "results-squad_v2.json",
        "dataset_name": "squad_v2",
        "rag_target": "standard",
        "category": "benchmark",
        "total_questions": 2370,
        "total_tested": 2370,
        "total_answered": 0,
        "total_errors": 2370,
        "avg_f1": 0.0,
        "answer_rate": "0.0%",
        "status": "ALL_ERRORED",
        "data_ready": null
      },
      {
        "file": "results-triviaqa.json",
        "dataset_name": "triviaqa",
        "rag_target": "standard",
        "category": "benchmark",
        "total_questions": 3250,
        "total_tested": 3250,
        "total_answered": 0,
        "total_errors": 3250,
        "avg_f1": 0.0,
        "answer_rate": "0.0%",
        "status": "ALL_ERRORED",
        "data_ready": null
      }
    ],
    "known_issues": [
      "All 28,053 questions errored with 'fetch is not defined'",
      "Bug was fixed in later workflow versions",
      "Need to re-run after fix"
    ]
  },
  "already_answered_positive": {
    "description": "Questions that received a valid answer with F1 > 0",
    "count": 0,
    "details": []
  },
  "action_items": {
    "priority_1": {
      "action": "Ingest Graph RAG data into Neo4j",
      "description": "Load musique and 2wikimultihopqa entity data with relationships into Neo4j. Without this data, graph RAG questions return 'context does not contain information'.",
      "datasets_affected": [
        "musique",
        "2wikimultihopqa"
      ],
      "questions_affected": 500
    },
    "priority_2": {
      "action": "Ingest Quantitative RAG data into Supabase",
      "description": "Load finqa, tatqa, convfinqa, wikitablequestions tabular data into SQL tables. Without this, Text-to-SQL returns NULL.",
      "datasets_affected": [
        "finqa",
        "tatqa",
        "convfinqa",
        "wikitablequestions"
      ],
      "questions_affected": 500
    },
    "priority_3": {
      "action": "Fix Graph RAG answer extraction",
      "description": "Graph RAG returns raw community summaries instead of LLM-generated answers. Fix the answer extraction node in WF2.",
      "workflow": "TEST - SOTA 2026 - WF2 Graph RAG V3.3"
    },
    "priority_4": {
      "action": "Re-run benchmark questions with fixed workflows",
      "description": "The fetch() bug is fixed. Re-run 28,053 benchmark questions through Standard RAG.",
      "questions_affected": 28053
    }
  },
  "per_dataset_breakdown": [
    {
      "file": "results-2wikimultihopqa.json",
      "dataset_name": "2wikimultihopqa",
      "rag_target": "graph",
      "category": "multi_hop_qa",
      "total_questions": 300,
      "total_tested": 0,
      "total_answered": 0,
      "total_errors": 0,
      "avg_f1": 0.0,
      "answer_rate": "0%",
      "status": "NOT_STARTED",
      "data_ready": false
    },
    {
      "file": "results-asqa.json",
      "dataset_name": "asqa",
      "rag_target": "standard",
      "category": "benchmark",
      "total_questions": 2750,
      "total_tested": 2750,
      "total_answered": 0,
      "total_errors": 2750,
      "avg_f1": 0.0,
      "answer_rate": "0.0%",
      "status": "ALL_ERRORED",
      "data_ready": null
    },
    {
      "file": "results-convfinqa.json",
      "dataset_name": "convfinqa",
      "rag_target": "quantitative",
      "category": "domain_finance",
      "total_questions": 100,
      "total_tested": 0,
      "total_answered": 0,
      "total_errors": 0,
      "avg_f1": 0.0,
      "answer_rate": "0%",
      "status": "NOT_STARTED",
      "data_ready": false
    },
    {
      "file": "results-finqa-quantitative.json",
      "dataset_name": "finqa",
      "rag_target": "quantitative",
      "category": "domain_finance",
      "total_questions": 200,
      "total_tested": 0,
      "total_answered": 0,
      "total_errors": 0,
      "avg_f1": 0.0,
      "answer_rate": "0%",
      "status": "NOT_STARTED",
      "data_ready": false
    },
    {
      "file": "results-finqa.json",
      "dataset_name": "finqa",
      "rag_target": "standard",
      "category": "benchmark",
      "total_questions": 2830,
      "total_tested": 2830,
      "total_answered": 0,
      "total_errors": 2830,
      "avg_f1": 0.0,
      "answer_rate": "0.0%",
      "status": "ALL_ERRORED",
      "data_ready": null
    },
    {
      "file": "results-frames.json",
      "dataset_name": "frames",
      "rag_target": "standard",
      "category": "benchmark",
      "total_questions": 1943,
      "total_tested": 1943,
      "total_answered": 0,
      "total_errors": 1943,
      "avg_f1": 0.0,
      "answer_rate": "0.0%",
      "status": "ALL_ERRORED",
      "data_ready": null
    },
    {
      "file": "results-hotpotqa.json",
      "dataset_name": "hotpotqa",
      "rag_target": "standard",
      "category": "benchmark",
      "total_questions": 3320,
      "total_tested": 3320,
      "total_answered": 0,
      "total_errors": 3320,
      "avg_f1": 0.0,
      "answer_rate": "0.0%",
      "status": "ALL_ERRORED",
      "data_ready": null
    },
    {
      "file": "results-msmarco.json",
      "dataset_name": "msmarco",
      "rag_target": "standard",
      "category": "benchmark",
      "total_questions": 5250,
      "total_tested": 5250,
      "total_answered": 0,
      "total_errors": 5250,
      "avg_f1": 0.0,
      "answer_rate": "0.0%",
      "status": "ALL_ERRORED",
      "data_ready": null
    },
    {
      "file": "results-musique.json",
      "dataset_name": "musique",
      "rag_target": "graph",
      "category": "multi_hop_qa",
      "total_questions": 200,
      "total_tested": 0,
      "total_answered": 0,
      "total_errors": 0,
      "avg_f1": 0.0,
      "answer_rate": "0%",
      "status": "NOT_STARTED",
      "data_ready": false
    },
    {
      "file": "results-narrativeqa.json",
      "dataset_name": "narrativeqa",
      "rag_target": "standard",
      "category": "benchmark",
      "total_questions": 2750,
      "total_tested": 2750,
      "total_answered": 0,
      "total_errors": 2750,
      "avg_f1": 0.0,
      "answer_rate": "0.0%",
      "status": "ALL_ERRORED",
      "data_ready": null
    },
    {
      "file": "results-popqa.json",
      "dataset_name": "popqa",
      "rag_target": "standard",
      "category": "benchmark",
      "total_questions": 2250,
      "total_tested": 2250,
      "total_answered": 0,
      "total_errors": 2250,
      "avg_f1": 0.0,
      "answer_rate": "0.0%",
      "status": "ALL_ERRORED",
      "data_ready": null
    },
    {
      "file": "results-pubmedqa.json",
      "dataset_name": "pubmedqa",
      "rag_target": "standard",
      "category": "benchmark",
      "total_questions": 1340,
      "total_tested": 1340,
      "total_answered": 0,
      "total_errors": 1340,
      "avg_f1": 0.0,
      "answer_rate": "0.0%",
      "status": "ALL_ERRORED",
      "data_ready": null
    },
    {
      "file": "results-squad_v2.json",
      "dataset_name": "squad_v2",
      "rag_target": "standard",
      "category": "benchmark",
      "total_questions": 2370,
      "total_tested": 2370,
      "total_answered": 0,
      "total_errors": 2370,
      "avg_f1": 0.0,
      "answer_rate": "0.0%",
      "status": "ALL_ERRORED",
      "data_ready": null
    },
    {
      "file": "results-tatqa.json",
      "dataset_name": "tatqa",
      "rag_target": "quantitative",
      "category": "domain_finance",
      "total_questions": 150,
      "total_tested": 0,
      "total_answered": 0,
      "total_errors": 0,
      "avg_f1": 0.0,
      "answer_rate": "0%",
      "status": "NOT_STARTED",
      "data_ready": false
    },
    {
      "file": "results-triviaqa.json",
      "dataset_name": "triviaqa",
      "rag_target": "standard",
      "category": "benchmark",
      "total_questions": 3250,
      "total_tested": 3250,
      "total_answered": 0,
      "total_errors": 3250,
      "avg_f1": 0.0,
      "answer_rate": "0.0%",
      "status": "ALL_ERRORED",
      "data_ready": null
    },
    {
      "file": "results-wikitablequestions.json",
      "dataset_name": "wikitablequestions",
      "rag_target": "quantitative",
      "category": "table_qa",
      "total_questions": 50,
      "total_tested": 0,
      "total_answered": 0,
      "total_errors": 0,
      "avg_f1": 0.0,
      "answer_rate": "0%",
      "status": "NOT_STARTED",
      "data_ready": false
    }
  ]
}