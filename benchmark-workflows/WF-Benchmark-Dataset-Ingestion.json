{
  "name": "BENCHMARK - Dataset Ingestion Pipeline",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "benchmark-ingest",
        "options": {
          "rawBody": true
        },
        "responseMode": "responseNode"
      },
      "id": "b1000001-0001-4000-a001-000000000001",
      "name": "Webhook: Ingest Dataset",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2.1,
      "position": [-2400, 300],
      "webhookId": "bench-ingest-001"
    },
    {
      "parameters": {
        "jsCode": "// Init Benchmark Ingestion Session\nconst body = $json.body || $json || {};\n\n// Validate required fields\nconst required = ['dataset_name'];\nfor (const field of required) {\n  if (!body[field]) throw new Error(`VALIDATION_ERROR: ${field} is required`);\n}\n\n// Dataset registry — maps names to HuggingFace endpoints and schemas\nconst DATASET_REGISTRY = {\n  // Single-Hop QA\n  'natural_questions': { hf_id: 'google-research-datasets/nq_open', category: 'single_hop_qa', q_field: 'question', a_field: 'answer', split_default: 'validation', subset: 'nq_open' },\n  'triviaqa': { hf_id: 'mandarjoshi/trivia_qa', category: 'single_hop_qa', q_field: 'question', a_field: 'answer.value', split_default: 'validation', subset: 'rc.nocontext' },\n  'squad_v2': { hf_id: 'rajpurkar/squad_v2', category: 'single_hop_qa', q_field: 'question', a_field: 'answers.text[0]', context_field: 'context', split_default: 'validation', subset: 'squad_v2' },\n  'popqa': { hf_id: 'akariasai/PopQA', category: 'single_hop_qa', q_field: 'question', a_field: 'possible_answers', split_default: 'test' },\n  'webquestions': { hf_id: 'web_questions', category: 'single_hop_qa', q_field: 'question', a_field: 'answers[0]', split_default: 'test' },\n  // Multi-Hop QA\n  'hotpotqa': { hf_id: 'hotpotqa/hotpot_qa', category: 'multi_hop_qa', q_field: 'question', a_field: 'answer', context_field: 'context', supporting_field: 'supporting_facts', split_default: 'validation', subset: 'distractor' },\n  'musique': { hf_id: 'StonyBrookNLP/musique', category: 'multi_hop_qa', q_field: 'question', a_field: 'answer', split_default: 'validation' },\n  '2wikimultihopqa': { hf_id: 'scholarly-shadows-syndicate/2wikimultihopqa', category: 'multi_hop_qa', q_field: 'question', a_field: 'answer', context_field: 'context', supporting_field: 'supporting_facts', split_default: 'validation' },\n  'multihop_rag': { hf_id: 'yixuantt/MultiHopRAG', category: 'multi_hop_qa', q_field: 'query', a_field: 'answer', split_default: 'test' },\n  // RAG Benchmarks\n  'frames': { hf_id: 'google/frames-benchmark', category: 'rag_benchmark', q_field: 'Prompt', a_field: 'Answer', split_default: 'test' },\n  'ragbench': { hf_id: 'rungalileo/ragbench', category: 'rag_benchmark', q_field: 'question', a_field: 'response', split_default: 'test' },\n  // Retrieval\n  'msmarco': { hf_id: 'microsoft/ms_marco', category: 'retrieval', q_field: 'query', a_field: 'answers[0]', context_field: 'passages', split_default: 'test', subset: 'v2.1' },\n  // Domain-Specific\n  'pubmedqa': { hf_id: 'qiaojin/PubMedQA', category: 'domain_medical', q_field: 'question', a_field: 'long_answer', context_field: 'context', split_default: 'train', subset: 'pqa_labeled' },\n  'finqa': { hf_id: 'dreamerdeo/finqa', category: 'domain_finance', q_field: 'question', a_field: 'answer', split_default: 'test' },\n  'cuad': { hf_id: 'theatticusproject/cuad-qa', category: 'domain_legal', q_field: 'question', a_field: 'answers.text[0]', context_field: 'context', split_default: 'test' },\n  'covidqa': { hf_id: 'covid_qa_deepset', category: 'domain_medical', q_field: 'question', a_field: 'answers.text[0]', context_field: 'context', split_default: 'test' },\n  'techqa': { hf_id: 'ibm/techqa', category: 'domain_technical', q_field: 'question', a_field: 'answer', split_default: 'test' },\n  // Long-Form\n  'asqa': { hf_id: 'din0s/asqa', category: 'long_form_qa', q_field: 'ambiguous_question', a_field: 'annotations.long_answer', split_default: 'dev' },\n  'narrativeqa': { hf_id: 'deepmind/narrativeqa', category: 'long_form_qa', q_field: 'question.text', a_field: 'answers[0].text', split_default: 'test' },\n  // Quantitative / Table QA\n  'tatqa': { hf_id: 'next-tat/TAT-QA', category: 'domain_finance', q_field: 'question', a_field: 'answer', split_default: 'test' },\n  'convfinqa': { hf_id: 'TheFinAI/ConvFinQA', category: 'domain_finance', q_field: 'question', a_field: 'answer', split_default: 'test' },\n  'wikitablequestions': { hf_id: 'Stanford/wikitablequestions', category: 'table_qa', q_field: 'question', a_field: 'answers', split_default: 'test' },\n  // Robustness\n  'nomiracle': { hf_id: 'Project_NoMIRACL', category: 'robustness', q_field: 'query', a_field: 'answer', split_default: 'test' }\n};\n\nconst dsName = body.dataset_name.toLowerCase();\nconst dsConfig = DATASET_REGISTRY[dsName];\nif (!dsConfig) {\n  throw new Error(`UNKNOWN_DATASET: ${dsName}. Available: ${Object.keys(DATASET_REGISTRY).join(', ')}`);\n}\n\nconst runId = `ingest-${dsName}-${Date.now()}-${Math.random().toString(36).substring(2, 8)}`;\nconst traceId = `tr-bench-ingest-${Date.now()}-${Math.random().toString(36).substring(2, 10)}`;\n\nreturn {\n  run_id: runId,\n  trace_id: traceId,\n  dataset_name: dsName,\n  dataset_config: dsConfig,\n  hf_id: dsConfig.hf_id,\n  hf_subset: dsConfig.subset || '',\n  category: dsConfig.category,\n  split: body.split || dsConfig.split_default,\n  sample_size: Math.min(body.sample_size || 100, 100),\n  hf_offset: body.hf_offset || 0,\n  batch_size: body.batch_size || 50,\n  target_dbs: body.target_dbs || ['supabase', 'pinecone'],\n  include_neo4j: body.include_neo4j === true,\n  tenant_id: body.tenant_id || 'benchmark',\n  generate_embeddings: body.generate_embeddings === true,\n  timestamp: new Date().toISOString()\n};"
      },
      "id": "b1000001-0002-4000-a001-000000000002",
      "name": "Init Ingestion Session",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [-2100, 300]
    },
    {
      "parameters": {
        "method": "GET",
        "url": "=https://datasets-server.huggingface.co/rows?dataset={{ $json.hf_id }}&config={{ $json.hf_subset || 'default' }}&split={{ $json.split }}&offset={{ $json.hf_offset || 0 }}&length={{ Math.min($json.sample_size, 100) }}",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Authorization",
              "value": "Bearer {{ $vars.HF_TOKEN }}"
            }
          ]
        },
        "options": {
          "timeout": 120000,
          "fullResponse": true
        }
      },
      "id": "b1000001-0003-4000-a001-000000000003",
      "name": "Fetch HuggingFace Dataset",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [-1800, 300],
      "onError": "continueErrorOutput"
    },
    {
      "parameters": {
        "jsCode": "// Parse and normalize dataset to unified schema\nconst initData = $node['Init Ingestion Session'].json;\nconst response = $json;\n\nif (!response.body && !response.rows) {\n  throw new Error(`HF_FETCH_ERROR: No data returned for ${initData.dataset_name}`);\n}\n\nconst rows = response.rows || response.body?.rows || [];\nif (rows.length === 0) {\n  throw new Error(`HF_EMPTY_DATASET: No rows for ${initData.dataset_name} (split: ${initData.split})`);\n}\n\nconst cfg = initData.dataset_config;\n\n// Deep field accessor: supports 'answers.text[0]', 'answer.value', etc.\nfunction getField(obj, path) {\n  if (!path) return null;\n  const parts = path.replace(/\\[(\\d+)\\]/g, '.$1').split('.');\n  let val = obj;\n  for (const p of parts) {\n    if (val == null) return null;\n    val = val[p];\n  }\n  // If result is array, join with ' | '\n  if (Array.isArray(val)) return val.join(' | ');\n  return val;\n}\n\n// Normalize each row\nconst hfOffset = initData.hf_offset || 0;\nconst items = rows.slice(0, initData.sample_size).map((row, idx) => {\n  const r = row.row || row;\n  return {\n    dataset_name: initData.dataset_name,\n    category: initData.category,\n    split: initData.split,\n    item_index: hfOffset + idx,\n    question: getField(r, cfg.q_field) || '',\n    expected_answer: getField(r, cfg.a_field) || '',\n    context: cfg.context_field ? getField(r, cfg.context_field) : null,\n    supporting_facts: cfg.supporting_field ? getField(r, cfg.supporting_field) : null,\n    metadata: {\n      original_id: r.id || r._id || idx,\n      type: r.type || null,\n      level: r.level || null,\n      source: initData.hf_id\n    },\n    tenant_id: initData.tenant_id,\n    batch_id: null  // will be assigned during batching\n  };\n}).filter(item => item.question && item.question.length > 0);\n\nconsole.log(`Parsed ${items.length} valid items from ${rows.length} rows`);\n\nreturn {\n  ...initData,\n  items: items,\n  total_items: items.length,\n  parse_stats: {\n    raw_rows: rows.length,\n    valid_items: items.length,\n    filtered_out: rows.length - items.length\n  }\n};"
      },
      "id": "b1000001-0004-4000-a001-000000000004",
      "name": "Parse & Normalize Dataset",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [-1500, 300]
    },
    {
      "parameters": {
        "jsCode": "// Create batches from items\nconst data = $json;\nconst batchSize = data.batch_size;\nconst items = data.items;\nconst batches = [];\n\nfor (let i = 0; i < items.length; i += batchSize) {\n  const batchId = `${data.run_id}-batch-${Math.floor(i / batchSize).toString().padStart(4, '0')}`;\n  const batchItems = items.slice(i, i + batchSize).map(item => ({\n    ...item,\n    batch_id: batchId\n  }));\n  batches.push({\n    batch_id: batchId,\n    batch_index: Math.floor(i / batchSize),\n    batch_size: batchItems.length,\n    items: batchItems,\n    run_id: data.run_id,\n    trace_id: data.trace_id,\n    dataset_name: data.dataset_name,\n    category: data.category,\n    tenant_id: data.tenant_id,\n    target_dbs: data.target_dbs,\n    include_neo4j: data.include_neo4j,\n    generate_embeddings: data.generate_embeddings,\n    total_batches: Math.ceil(items.length / batchSize),\n    total_items: data.total_items\n  });\n}\n\nconsole.log(`Created ${batches.length} batches of ~${batchSize} items`);\n\n// Return as array for SplitInBatches\nreturn batches.map(b => ({ json: b }));"
      },
      "id": "b1000001-0005-4000-a001-000000000005",
      "name": "Split Into Batches",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [-1200, 300]
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "=INSERT INTO benchmark_runs (run_id, run_type, phase, workflow_name, dataset_names, config, status, total_items, tenant_id, trace_id, triggered_by)\nVALUES (\n  '{{ $node['Init Ingestion Session'].json.run_id }}',\n  'ingestion',\n  'phase_1',\n  'BENCHMARK - Dataset Ingestion Pipeline',\n  ARRAY['{{ $node['Init Ingestion Session'].json.dataset_name }}'],\n  '{{ JSON.stringify({ batch_size: $node['Init Ingestion Session'].json.batch_size, sample_size: $node['Init Ingestion Session'].json.sample_size, target_dbs: $node['Init Ingestion Session'].json.target_dbs }) }}'::jsonb,\n  'running',\n  {{ $node['Parse & Normalize Dataset'].json.total_items }},\n  '{{ $node['Init Ingestion Session'].json.tenant_id }}',\n  '{{ $node['Init Ingestion Session'].json.trace_id }}',\n  'manual'\n)\nON CONFLICT (run_id) DO UPDATE SET status = 'running', started_at = NOW();"
      },
      "id": "b1000001-0006-4000-a001-000000000006",
      "name": "Log Run Start",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.5,
      "position": [-1200, 560],
      "credentials": {
        "postgres": {
          "id": "zEr7jPswZNv6lWKu",
          "name": "Supabase PostgreSQL"
        }
      }
    },
    {
      "parameters": {
        "batchSize": 1,
        "options": {}
      },
      "id": "b1000001-0007-4000-a001-000000000007",
      "name": "Loop Over Batches",
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 3,
      "position": [-900, 300]
    },
    {
      "parameters": {
        "jsCode": "// Store batch items into Supabase benchmark_datasets table\nconst batch = $json;\nconst items = batch.items;\n\n// Build multi-row INSERT values\nconst values = items.map(item => {\n  const q = (item.question || '').replace(/'/g, \"''\");\n  const a = (item.expected_answer || '').replace(/'/g, \"''\");\n  const ctx = item.context ? `'${(typeof item.context === 'string' ? item.context : JSON.stringify(item.context)).replace(/'/g, \"''\")}'` : 'NULL';\n  const sf = item.supporting_facts ? `'${JSON.stringify(item.supporting_facts).replace(/'/g, \"''\")}'::jsonb` : 'NULL';\n  const meta = `'${JSON.stringify(item.metadata).replace(/'/g, \"''\")}'::jsonb`;\n  return `('${item.dataset_name}', '${item.category}', '${item.split}', ${item.item_index}, '${q}', '${a}', ${ctx}, ${sf}, ${meta}, '${item.tenant_id}', '${item.batch_id}')`;\n}).join(',\\n');\n\nconst sql = `INSERT INTO benchmark_datasets (dataset_name, category, split, item_index, question, expected_answer, context, supporting_facts, metadata, tenant_id, batch_id)\nVALUES ${values}\nON CONFLICT (dataset_name, split, item_index, tenant_id) DO UPDATE SET\n  question = EXCLUDED.question,\n  expected_answer = EXCLUDED.expected_answer,\n  context = EXCLUDED.context,\n  metadata = EXCLUDED.metadata,\n  batch_id = EXCLUDED.batch_id,\n  ingested_at = NOW();`;\n\nreturn {\n  ...batch,\n  supabase_sql: sql,\n  supabase_count: items.length\n};"
      },
      "id": "b1000001-0008-4000-a001-000000000008",
      "name": "Prepare Supabase Insert",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [-600, 200]
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "={{ $json.supabase_sql }}"
      },
      "id": "b1000001-0009-4000-a001-000000000009",
      "name": "Supabase: Store Q&A",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.5,
      "position": [-300, 200],
      "credentials": {
        "postgres": {
          "id": "zEr7jPswZNv6lWKu",
          "name": "Supabase PostgreSQL"
        }
      },
      "onError": "continueErrorOutput"
    },
    {
      "parameters": {
        "conditions": {
          "options": { "caseSensitive": true, "typeValidation": "strict" },
          "conditions": [
            {
              "id": "check-embeddings",
              "leftValue": "={{ $json.generate_embeddings }}",
              "rightValue": true,
              "operator": { "type": "boolean", "operation": "equals" }
            }
          ]
        }
      },
      "id": "b1000001-0010-4000-a001-000000000010",
      "name": "Generate Embeddings?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [0, 200]
    },
    {
      "parameters": {
        "jsCode": "// Prepare embedding requests for each question in the batch\nconst batch = $json;\nconst items = batch.items;\n\n// Build texts to embed: question + optional context snippet\nconst texts = items.map(item => {\n  let text = item.question;\n  if (item.context && typeof item.context === 'string') {\n    text += '\\n\\nContext: ' + item.context.substring(0, 500);\n  }\n  return text;\n});\n\nreturn {\n  ...batch,\n  embed_texts: texts,\n  embed_count: texts.length\n};"
      },
      "id": "b1000001-0011-4000-a001-000000000011",
      "name": "Prepare Embedding Input",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [300, 100]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "={{ $vars.EMBEDDING_API_URL || 'https://api.openai.com/v1/embeddings' }}",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Authorization",
              "value": "Bearer {{ $vars.EMBEDDING_API_KEY || $credentials.openai_api?.apiKey }}"
            },
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={\n  \"model\": \"{{ $vars.EMBEDDING_MODEL || 'text-embedding-3-small' }}\",\n  \"input\": {{ JSON.stringify($json.embed_texts) }}\n}",
        "options": {
          "timeout": 60000
        }
      },
      "id": "b1000001-0012-4000-a001-000000000012",
      "name": "Generate Embeddings",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [600, 100],
      "onError": "continueErrorOutput"
    },
    {
      "parameters": {
        "jsCode": "// Prepare Pinecone upsert with vectors\nconst batch = $node['Prepare Embedding Input'].json;\nconst embeddingResponse = $json;\n\nconst embeddings = embeddingResponse.body?.data || embeddingResponse.data || [];\nconst items = batch.items;\n\nconst vectors = items.map((item, idx) => {\n  const embedding = embeddings[idx];\n  const vectorId = `bench-${item.dataset_name}-${item.split}-${item.item_index}`;\n  \n  return {\n    id: vectorId,\n    values: embedding?.embedding || [],\n    metadata: {\n      dataset_name: item.dataset_name,\n      category: item.category,\n      split: item.split,\n      item_index: item.item_index,\n      question: item.question.substring(0, 500),\n      expected_answer: (item.expected_answer || '').substring(0, 500),\n      tenant_id: item.tenant_id,\n      ingested_at: new Date().toISOString()\n    }\n  };\n}).filter(v => v.values && v.values.length > 0);\n\nreturn {\n  ...batch,\n  pinecone_vectors: vectors,\n  pinecone_count: vectors.length,\n  embedding_dimension: vectors[0]?.values?.length || 0\n};"
      },
      "id": "b1000001-0013-4000-a001-000000000013",
      "name": "Prepare Pinecone Vectors",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [900, 100]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "={{ $vars.PINECONE_HOST || 'https://n8nultimate-a4mkzmz.svc.aped-4627-b74a.pinecone.io' }}/vectors/upsert",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Api-Key",
              "value": "={{ $vars.PINECONE_API_KEY }}"
            },
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={\n  \"vectors\": {{ JSON.stringify($json.pinecone_vectors) }},\n  \"namespace\": \"benchmark-{{ $json.dataset_name }}\"\n}",
        "options": {
          "timeout": 60000
        }
      },
      "id": "b1000001-0014-4000-a001-000000000014",
      "name": "Pinecone: Upsert Vectors",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [1200, 100],
      "onError": "continueErrorOutput"
    },
    {
      "parameters": {
        "conditions": {
          "options": { "caseSensitive": true, "typeValidation": "strict" },
          "conditions": [
            {
              "id": "check-neo4j",
              "leftValue": "={{ $json.include_neo4j }}",
              "rightValue": true,
              "operator": { "type": "boolean", "operation": "equals" }
            }
          ]
        }
      },
      "id": "b1000001-0015-4000-a001-000000000015",
      "name": "Include Neo4j?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [0, 420]
    },
    {
      "parameters": {
        "jsCode": "// Build Cypher queries for Neo4j ingestion (multi-hop datasets)\nconst batch = $json;\nconst items = batch.items;\n\n// Create nodes for questions and their relationships\nconst cypherStatements = items.map(item => {\n  const q = (item.question || '').replace(/'/g, \"\\\\'\").replace(/\\\\/g, '\\\\\\\\');\n  const a = (item.expected_answer || '').replace(/'/g, \"\\\\'\").replace(/\\\\/g, '\\\\\\\\');\n  \n  return `MERGE (q:BenchmarkQuestion {dataset: '${item.dataset_name}', idx: ${item.item_index}})\n  SET q.question = '${q}',\n      q.expected_answer = '${a}',\n      q.category = '${item.category}',\n      q.split = '${item.split}',\n      q.tenant_id = '${item.tenant_id}',\n      q.ingested_at = datetime()`;\n});\n\n// Batch all statements into one transaction\nconst batchCypher = cypherStatements.join('\\n');\n\nreturn {\n  ...batch,\n  cypher_query: batchCypher,\n  neo4j_count: items.length\n};"
      },
      "id": "b1000001-0016-4000-a001-000000000016",
      "name": "Prepare Neo4j Cypher",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [300, 420]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "={{ $vars.NEO4J_HTTP_URL || 'https://a9a062c3.databases.neo4j.io:7687' }}/db/neo4j/tx/commit",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Authorization",
              "value": "Basic {{ Buffer.from(($vars.NEO4J_USER || 'neo4j') + ':' + $vars.NEO4J_PASSWORD).toString('base64') }}"
            },
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={\n  \"statements\": [{\n    \"statement\": {{ JSON.stringify($json.cypher_query) }}\n  }]\n}",
        "options": {
          "timeout": 30000
        }
      },
      "id": "b1000001-0017-4000-a001-000000000017",
      "name": "Neo4j: Store Graph Nodes",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [600, 420],
      "onError": "continueErrorOutput"
    },
    {
      "parameters": {
        "jsCode": "// Batch progress logger — update ingestion stats\nconst batch = $json;\nconst batchIndex = batch.batch_index;\nconst totalBatches = batch.total_batches;\nconst batchSize = batch.batch_size;\nconst processedSoFar = (batchIndex + 1) * batch.batch_size;\nconst pct = Math.round((processedSoFar / batch.total_items) * 100);\n\nconsole.log(`[BENCHMARK INGEST] ${batch.dataset_name} — Batch ${batchIndex + 1}/${totalBatches} (${pct}%)`);\nconsole.log(`  Supabase: ${batch.supabase_count || batchSize} rows`);\nconsole.log(`  Pinecone: ${batch.pinecone_count || 0} vectors`);\nconsole.log(`  Neo4j: ${batch.neo4j_count || 0} nodes`);\n\nreturn {\n  batch_index: batchIndex,\n  total_batches: totalBatches,\n  progress_pct: pct,\n  processed_items: Math.min(processedSoFar, batch.total_items),\n  dataset_name: batch.dataset_name,\n  run_id: batch.run_id,\n  trace_id: batch.trace_id,\n  batch_stats: {\n    supabase_rows: batch.supabase_count || batchSize,\n    pinecone_vectors: batch.pinecone_count || 0,\n    neo4j_nodes: batch.neo4j_count || 0\n  }\n};"
      },
      "id": "b1000001-0018-4000-a001-000000000018",
      "name": "Batch Progress Logger",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1500, 300]
    },
    {
      "parameters": {
        "jsCode": "// Final summary — aggregate all batch results\nconst initData = $node['Init Ingestion Session'].json;\nconst parseData = $node['Parse & Normalize Dataset'].json;\n\nconst summary = {\n  run_id: initData.run_id,\n  trace_id: initData.trace_id,\n  dataset_name: initData.dataset_name,\n  category: initData.category,\n  split: initData.split,\n  total_items: parseData.total_items,\n  parse_stats: parseData.parse_stats,\n  target_dbs: initData.target_dbs,\n  include_neo4j: initData.include_neo4j,\n  generate_embeddings: initData.generate_embeddings,\n  batch_size: initData.batch_size,\n  total_batches: Math.ceil(parseData.total_items / initData.batch_size),\n  status: 'completed',\n  completed_at: new Date().toISOString(),\n  started_at: initData.timestamp\n};\n\n// Calculate duration\nconst startMs = new Date(summary.started_at).getTime();\nconst endMs = new Date(summary.completed_at).getTime();\nsummary.duration_ms = endMs - startMs;\nsummary.duration_human = `${Math.round(summary.duration_ms / 1000)}s`;\n\nreturn summary;"
      },
      "id": "b1000001-0019-4000-a001-000000000019",
      "name": "Final Summary",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1800, 300]
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "=UPDATE benchmark_runs SET\n  status = 'completed',\n  processed_items = {{ $json.total_items }},\n  completed_at = NOW(),\n  duration_ms = {{ $json.duration_ms }}\nWHERE run_id = '{{ $json.run_id }}';\n\nINSERT INTO benchmark_ingestion_stats (dataset_name, split, total_items, ingested_items, pinecone_vectors, supabase_rows, last_batch_id, status, started_at, completed_at)\nVALUES ('{{ $json.dataset_name }}', '{{ $json.split }}', {{ $json.total_items }}, {{ $json.total_items }}, {{ $json.generate_embeddings ? $json.total_items : 0 }}, {{ $json.total_items }}, '{{ $json.run_id }}', 'completed', '{{ $json.started_at }}', NOW())\nON CONFLICT (dataset_name, split) DO UPDATE SET\n  ingested_items = EXCLUDED.ingested_items,\n  pinecone_vectors = EXCLUDED.pinecone_vectors,\n  supabase_rows = EXCLUDED.supabase_rows,\n  last_batch_id = EXCLUDED.last_batch_id,\n  status = 'completed',\n  completed_at = NOW();"
      },
      "id": "b1000001-0020-4000-a001-000000000020",
      "name": "Log Run Completed",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.5,
      "position": [2100, 300],
      "credentials": {
        "postgres": {
          "id": "zEr7jPswZNv6lWKu",
          "name": "Supabase PostgreSQL"
        }
      }
    },
    {
      "parameters": {
        "method": "POST",
        "url": "={{ $vars.OTEL_EXPORTER_URL || 'https://otel-collector.internal:4318' }}/v1/traces",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={\n  \"resourceSpans\": [{\n    \"resource\": { \"attributes\": [{ \"key\": \"service.name\", \"value\": { \"stringValue\": \"benchmark-ingestion\" } }] },\n    \"scopeSpans\": [{\n      \"spans\": [{\n        \"traceId\": \"{{ $json.trace_id }}\",\n        \"spanId\": \"{{ $json.run_id }}\",\n        \"name\": \"benchmark.ingest.{{ $json.dataset_name }}\",\n        \"kind\": 1,\n        \"startTimeUnixNano\": \"{{ new Date($json.started_at).getTime() * 1000000 }}\",\n        \"endTimeUnixNano\": \"{{ Date.now() * 1000000 }}\",\n        \"attributes\": [\n          { \"key\": \"dataset\", \"value\": { \"stringValue\": \"{{ $json.dataset_name }}\" } },\n          { \"key\": \"items\", \"value\": { \"intValue\": {{ $json.total_items }} } },\n          { \"key\": \"duration_ms\", \"value\": { \"intValue\": {{ $json.duration_ms }} } },\n          { \"key\": \"status\", \"value\": { \"stringValue\": \"{{ $json.status }}\" } }\n        ],\n        \"status\": { \"code\": 1 }\n      }]\n    }]\n  }]\n}",
        "options": {
          "timeout": 10000
        }
      },
      "id": "b1000001-0021-4000-a001-000000000021",
      "name": "Export Trace OTEL",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [2400, 300],
      "onError": "continueErrorOutput"
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ JSON.stringify({ success: true, run_id: $json.run_id, dataset: $json.dataset_name, total_items: $json.total_items, duration: $json.duration_human, status: $json.status }) }}"
      },
      "id": "b1000001-0022-4000-a001-000000000022",
      "name": "Respond Success",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.1,
      "position": [2700, 300]
    },
    {
      "parameters": {
        "jsCode": "// Merge results from parallel DB operations back to batch loop\nreturn $json;"
      },
      "id": "b1000001-0023-4000-a001-000000000023",
      "name": "Merge DB Results",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1200, 300]
    }
  ],
  "connections": {
    "Webhook: Ingest Dataset": {
      "main": [
        [
          { "node": "Init Ingestion Session", "type": "main", "index": 0 }
        ]
      ]
    },
    "Init Ingestion Session": {
      "main": [
        [
          { "node": "Fetch HuggingFace Dataset", "type": "main", "index": 0 }
        ]
      ]
    },
    "Fetch HuggingFace Dataset": {
      "main": [
        [
          { "node": "Parse & Normalize Dataset", "type": "main", "index": 0 }
        ]
      ]
    },
    "Parse & Normalize Dataset": {
      "main": [
        [
          { "node": "Split Into Batches", "type": "main", "index": 0 },
          { "node": "Log Run Start", "type": "main", "index": 0 }
        ]
      ]
    },
    "Split Into Batches": {
      "main": [
        [
          { "node": "Loop Over Batches", "type": "main", "index": 0 }
        ]
      ]
    },
    "Loop Over Batches": {
      "main": [
        [
          { "node": "Final Summary", "type": "main", "index": 0 }
        ],
        [
          { "node": "Prepare Supabase Insert", "type": "main", "index": 0 },
          { "node": "Generate Embeddings?", "type": "main", "index": 0 },
          { "node": "Include Neo4j?", "type": "main", "index": 0 }
        ]
      ]
    },
    "Prepare Supabase Insert": {
      "main": [
        [
          { "node": "Supabase: Store Q&A", "type": "main", "index": 0 }
        ]
      ]
    },
    "Supabase: Store Q&A": {
      "main": [
        [
          { "node": "Merge DB Results", "type": "main", "index": 0 }
        ]
      ]
    },
    "Generate Embeddings?": {
      "main": [
        [
          { "node": "Prepare Embedding Input", "type": "main", "index": 0 }
        ],
        [
          { "node": "Merge DB Results", "type": "main", "index": 0 }
        ]
      ]
    },
    "Prepare Embedding Input": {
      "main": [
        [
          { "node": "Generate Embeddings", "type": "main", "index": 0 }
        ]
      ]
    },
    "Generate Embeddings": {
      "main": [
        [
          { "node": "Prepare Pinecone Vectors", "type": "main", "index": 0 }
        ]
      ]
    },
    "Prepare Pinecone Vectors": {
      "main": [
        [
          { "node": "Pinecone: Upsert Vectors", "type": "main", "index": 0 }
        ]
      ]
    },
    "Pinecone: Upsert Vectors": {
      "main": [
        [
          { "node": "Merge DB Results", "type": "main", "index": 0 }
        ]
      ]
    },
    "Include Neo4j?": {
      "main": [
        [
          { "node": "Prepare Neo4j Cypher", "type": "main", "index": 0 }
        ],
        [
          { "node": "Merge DB Results", "type": "main", "index": 0 }
        ]
      ]
    },
    "Prepare Neo4j Cypher": {
      "main": [
        [
          { "node": "Neo4j: Store Graph Nodes", "type": "main", "index": 0 }
        ]
      ]
    },
    "Neo4j: Store Graph Nodes": {
      "main": [
        [
          { "node": "Merge DB Results", "type": "main", "index": 0 }
        ]
      ]
    },
    "Merge DB Results": {
      "main": [
        [
          { "node": "Batch Progress Logger", "type": "main", "index": 0 }
        ]
      ]
    },
    "Batch Progress Logger": {
      "main": [
        [
          { "node": "Loop Over Batches", "type": "main", "index": 0 }
        ]
      ]
    },
    "Final Summary": {
      "main": [
        [
          { "node": "Log Run Completed", "type": "main", "index": 0 }
        ]
      ]
    },
    "Log Run Completed": {
      "main": [
        [
          { "node": "Export Trace OTEL", "type": "main", "index": 0 }
        ]
      ]
    },
    "Export Trace OTEL": {
      "main": [
        [
          { "node": "Respond Success", "type": "main", "index": 0 }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1",
    "callerPolicy": "workflowsFromSameOwner",
    "availableInMCP": true,
    "timeSavedMode": "fixed"
  },
  "meta": {
    "instanceId": "810d143e45edb75891ee3244decd00dc613435a73f5b3ad2900fe9bc764e9d73"
  },
  "tags": [
    { "name": "benchmark" },
    { "name": "ingestion" },
    { "name": "phase-1" }
  ]
}
